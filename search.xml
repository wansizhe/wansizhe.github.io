<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>阿里巴巴CVR预估模型：ESMM</title>
    <url>/2020/12/01/Alibaba-ESMM/</url>
    <content><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1804.07931">Entire Space Multi-Task Model: An Eﬀective Approach for Estimating Post-Click Conversion Rate</a></p>
<a id="more"></a>
<h2 id="0x00-CVR与CTR的区别"><a href="#0x00-CVR与CTR的区别" class="headerlink" title="0x00 CVR与CTR的区别"></a>0x00 CVR与CTR的区别</h2><p>CTR是点击率，CVR是转化率，点击率问题非常直接，其含义只有两步：</p>
<ol>
<li>页面曝光</li>
<li>用户点击</li>
</ol>
<p>转化率中涉及的一般是转化为收益的行为，例如淘宝场景中的下单购买。转化率问题在业务逻辑上没有点击率问题那么简单，通常包含三步：</p>
<ol>
<li>页面曝光</li>
<li>用户点击</li>
<li>用户购买</li>
</ol>
<p><img src="/2020/12/01/Alibaba-ESMM/CTR和CVR的样本关系.png" alt="CTR和CVR的样本关系"></p>
<p>转化行为发生前，一定会出现点击行为，这是不可避免的。但是CVR预估问题，最终预估的概率不是“点击且转化”（这是CTCVR问题），而是“点击条件下转化”，这之间的区别和联系可以用下面这个式子表达</p>
<script type="math/tex; mode=display">
P\left(click,conversion\right) = P\left(click\right)P\left(conversion|click\right)</script><p>也就是</p>
<script type="math/tex; mode=display">
P_{CTCVR} = P_{CTR}\cdot P_{CVR}</script><p>所以不能使用全部的样本进行训练，假设样本由特征和标签组成，标签为1，代表成功转化，样本特征如果不做特别注明，那么就无法知道样本是否被点击过，这样训练的模型预测的是CTCVR，而不是CVR。联系实际场景，很多商品你没有购买的直接原因是你没有点击它，CVR预估模型需要预测一个物品假设你点击了，买的概率有多大。</p>
<p>从以上问题中梳理出以下两个重要问题：</p>
<ol>
<li>Sample Selection Bias（SSB）。通常CVR预估问题在模型训练时，提供的样本局限于被点击的记录，其中成功转化的作为正样本，没有转化的作为负样本。但是在预测时，是对所有样本进行预测，而不是对被点击的样本进行预测。样本的选择造成了模型的偏差。</li>
<li>Data Sparsity（DS）。当样本被局限于点击过的数据时，样本量会骤减，远不如CTR问题中曝光的数据量。</li>
</ol>
<h2 id="0x01-模型"><a href="#0x01-模型" class="headerlink" title="0x01 模型"></a>0x01 模型</h2><p><img src="/2020/12/01/Alibaba-ESMM/ESMM.png" alt="ESMM模型结构图"></p>
<p>这个模型体现出了CTCVR、CTR、CVR三者的关系，左边负责预测CVR，右边负责预测CTR，顶部通过相乘预测CTCVR。左侧CVR预估模型不直接参加计算损失函数，损失函数只包含CTR和CTCVR两部分</p>
<script type="math/tex; mode=display">
Loss\left(\theta_{ctr},\theta_{cvr}\right) = 
\sum_{i=1}^{N}{L\left(y_{i},f\left(x_{i};\theta_{ctr}\right)\right)}+
\sum_{i=1}^{N}{L\left(y_{i}\&z_{i},f\left(x_{i};\theta_{ctr}\right)\cdot f\left(x_{i};\theta_{cvr}\right)\right)}</script><p>这样设计，是因为CTR预估任务和CTCVR预估任务可以使用全部的样本，通过“曲线救国”的方式，绕开了CVR预估的样本问题。</p>
<h2 id="0x02-实验"><a href="#0x02-实验" class="headerlink" title="0x02 实验"></a>0x02 实验</h2><p>作者对比的模型如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">结构说明（逐渐复杂）</th>
<th style="text-align:center">训练样本（逐渐扩充）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">BASE</td>
<td style="text-align:center">仅CVR部分</td>
<td style="text-align:center">仅包含点击样本</td>
</tr>
<tr>
<td style="text-align:center">AMAN</td>
<td style="text-align:center">同上</td>
<td style="text-align:center">从未点击样本中采样作为负例加入</td>
</tr>
<tr>
<td style="text-align:center">OVERSAMPLING</td>
<td style="text-align:center">同上</td>
<td style="text-align:center">对正例（点击并转化）过采样</td>
</tr>
<tr>
<td style="text-align:center">UNBIAS</td>
<td style="text-align:center">同上</td>
<td style="text-align:center">加入rejection sampling</td>
</tr>
<tr>
<td style="text-align:center">DIVISION</td>
<td style="text-align:center">分别训练CTR和CTCVR，相除得CVR</td>
<td style="text-align:center">同上</td>
</tr>
<tr>
<td style="text-align:center">ESMM-NS</td>
<td style="text-align:center">ESMM模型，embedding层不共享参数</td>
<td style="text-align:center">同上</td>
</tr>
<tr>
<td style="text-align:center">ESMM</td>
<td style="text-align:center">完整的ESMM模型</td>
<td style="text-align:center">同上</td>
</tr>
</tbody>
</table>
</div>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/05/13/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Introduction</category>
      </categories>
  </entry>
  <entry>
    <title>my first blog</title>
    <url>/2020/05/13/my-first-blog/</url>
    <content><![CDATA[<h3 id="this-is-my-first-hexo-blog"><a href="#this-is-my-first-hexo-blog" class="headerlink" title="this is my first hexo blog"></a>this is my first hexo blog</h3><a id="more"></a>
<p>test</p>
<script type="math/tex; mode=display">
f(x)=\frac{P(x)}{Q(x)}</script>]]></content>
      <categories>
        <category>Introduction</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机（SVM）数学推导思路</title>
    <url>/2020/06/03/SVM-math/</url>
    <content><![CDATA[<h1 id="相关定义和说明"><a href="#相关定义和说明" class="headerlink" title="相关定义和说明"></a>相关定义和说明</h1><p>数据集定义为： $\left\{ (x_{i}, y_{i}) \right\}_{i=1}^{N}$ ，其中 $x_{i}\in \mathbb{R}^{p}$ ， $y_{i} \in \left\{ -1, +1 \right\}$ </p>
<p>模型定义为： $f\left( x \right) = \text{sign}\left( w^{T} x + b \right)$ ，其中 $w \in \mathbb{R}^{p}$ </p>
<a id="more"></a>
<p>整体知识框架为</p>
<ul>
<li>Hard-Margin</li>
<li>Soft-Margin</li>
</ul>
<p>具体到以上两部分推导的内部，运用到了</p>
<ul>
<li>间隔</li>
<li>对偶</li>
<li>核技巧（选择性使用）</li>
</ul>
<h2 id="Hard-Margin-硬间隔"><a href="#Hard-Margin-硬间隔" class="headerlink" title="Hard-Margin 硬间隔"></a>Hard-Margin 硬间隔</h2><p>设 $\text{margin}\left( w, b \right)$ 表示给定模型参数的情况下，所有样本到分类超平面的最小距离</p>
<script type="math/tex; mode=display">
\text{margin}\left( w, b \right)=\min distance\left( w, b, x_{i} \right)=\min\frac{1}{\left| \left| w \right| \right|}\left| w^{T}x+b \right|=\min\frac{1}{\left| \left| w \right| \right|}y_{i}\left( w^{T}x+b \right)</script><p>目标是将这个值最大化，优化目标为</p>
<script type="math/tex; mode=display">
\max_{w, b}\min_{x_{i}}\ \frac{1}{\left| \left| w \right| \right|}y_{i}\left( w^{T}x+b \right) \qquad \text{s.t.}\quad y_{i}\left( w^{T} x_{i}+b \right)> 0</script><p>令 $y_{i}\left( w^{T} x_{i}+b \right)\geq 1$ ，则优化目标变为</p>
<script type="math/tex; mode=display">
\max_{w, b}\ \frac{1}{\left| \left| w \right| \right|} \qquad \text{s.t.} \quad y_{i}\left( w^{T} x_{i}+b \right)\geq 1</script><p>等价于</p>
<script type="math/tex; mode=display">
\min_{w, b}\frac{1}{2}w^{T}w \qquad \text{s.t.} \quad y_{i}\left( w^{T} x_{i}+b \right)\geq 1</script><p>接下来试图让模型参数不受可行条件约束，该优化问题的拉格朗日函数为</p>
<script type="math/tex; mode=display">
L\left( w, b, \lambda \right)=\frac{1}{2}w^{T}w + \sum_{i=1}^{N}{\lambda_{i}\left[1- y_{i}\left( w^{T}x_{i}+b \right) \right]}</script><p>其中 $\lambda\in \mathbb{R}^p$ 且 $\lambda_{i}\geq 0$ </p>
<p>根据广义拉格朗日函数，优化目标等价于（证明略）</p>
<script type="math/tex; mode=display">
\min_{w, b}\max_{\lambda}\ L\left( w, b, \lambda \right) \qquad \text{s.t.} \quad \lambda_{i}\geq 0</script><p>接下来用求导方法求解 $\min_{w, b}\ L\left( w, b, \lambda \right)$</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial b} = 0\quad \Rightarrow \quad\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>代入拉格朗日函数后继续求导</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w} = 0\quad \Rightarrow \quad w=\sum_{i=1}^{N}{\lambda_{i}y_{i}x_{i}}</script><p>继续带入拉格朗日函数，求得 $\min_{w, b}\ L\left( w, b, \lambda \right)$</p>
<script type="math/tex; mode=display">
L\left( w, b, \lambda \right)=-\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}x_{i}^{T}x_{j}}}+\sum_{i=1}^{N}{\lambda_{i}}</script><p>代入对偶问题的优化目标，求这个最小值关于 $\lambda$ 的最大值，优化目标可以写成（下方通过提出负号，将最大化写为最小化）</p>
<script type="math/tex; mode=display">
\min_{\lambda}\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}x_{i}^{T}x_{j}}}-\sum_{i=1}^{N}{\lambda_{i}} \qquad \text{s.t.} \quad \lambda_{i}\geq 0，\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>求解该问题（可以用SMO算法）可以得到最优解 $\lambda^{\ast} = \left( \lambda_{1}^{\ast},\lambda_{1}^{\ast},…,\lambda_{N}^{\ast} \right)^{T}$ </p>
<p>接下来根据KKT条件求解模型参数的最优解。KKT条件包括三类条件</p>
<ul>
<li>求导为0条件： $\frac{\partial L}{\partial w}=0，\frac{\partial L}{\partial b}=0，\frac{\partial L}{\partial \lambda}=0$ </li>
<li>可行条件： $\lambda_{i}^{\ast}\geq 0，1-y_{i}\left( w^{T}x_{i}+b \right)\leq 0$ </li>
<li>松弛互补条件： $\lambda_{i}^{\ast}\left[ 1-y_{i}\left( w^{T}x_{i}+b \right) \right]=0$ </li>
</ul>
<p>根据KKT条件中的松弛互补条件，求解得</p>
<script type="math/tex; mode=display">
w^{\ast}=\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}}\\
b^{\ast}=y_{k}-w^{\ast T}x_{k}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}x^{k}}</script><p>其中样本 $\left( x_{k}, y_{k} \right)$ 满足 $1-y_{i}\left( w^{T}x_{i}+b \right)= 0，\lambda_{k} &gt;0$ ，这个样本就是支持向量</p>
<p>最终，决策函数为</p>
<script type="math/tex; mode=display">
f\left( x \right) = sign\left( \sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}}x  + b^{\ast} \right)</script><h2 id="Soft-Margin-软间隔"><a href="#Soft-Margin-软间隔" class="headerlink" title="Soft-Margin 软间隔"></a>Soft-Margin 软间隔</h2><p>与Hard-Margin相比，可以允许一点点错误，优化目标可以写为</p>
<script type="math/tex; mode=display">
\min_{w, b}\frac{1}{2}w^{T}w+\text{loss}</script><p>其中的损失函数为Hinge-Loss，即合页损失函数</p>
<script type="math/tex; mode=display">
\text{loss} = \sum_{i=1}^{N}{\max\left\{ 0,\ 1-y_{i}\left( w^{T}x_{i}+b \right)  \right\}}</script><p>令 $\xi_{i} = 1-y_{i}\left( w^{T}x_{i}+b \right)$ ，$\xi_{i}\geq 0$ ，则优化目标等价于</p>
<script type="math/tex; mode=display">
\min_{w, b}\frac{1}{2}w^{T}w+C\sum_{i=1}^{N}{\xi_{i}} \qquad \text{s.t.}\quad y_{i}\left( w^{T}x_{i}+b \right)\geq1-\xi_{i}，\xi_{i}\geq0</script><p>其中，$C$ 为超参数。与Hard-Margin的推导类似，最终优化目标等价于</p>
<script type="math/tex; mode=display">
\min_{\lambda}\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}x_{i}^{T}x_{j}}}-\sum_{i=1}^{N}{\lambda_{i}} \qquad \text{s.t.} \quad0\leq \lambda_{i}\leq C，\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>通过对 $\lambda$ 添加上限，影响了支持向量，影响了最优解。同样方法求得</p>
<script type="math/tex; mode=display">
w^{\ast}=\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}}\\
b^{\ast}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}x_{k}}</script><p>最终决策函数为</p>
<script type="math/tex; mode=display">
f\left( x \right) = \text{sign}\left( \sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}}x  + b^{\ast} \right)</script><h2 id="Kernel-核函数"><a href="#Kernel-核函数" class="headerlink" title="Kernel 核函数"></a>Kernel 核函数</h2><p>将普通SVM中的 $x$ 进行替换， $x\rightarrow\phi\left( x \right)$ ，核函数 $K\left( x, z \right)=\phi\left( x \right)^{T}\phi\left( z \right)$ ，以Soft-Margin为例，优化目标等价于</p>
<script type="math/tex; mode=display">
\min_{\lambda}\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}K\left( x_{i}, x_{k} \right)}}-\sum_{i=1}^{N}{\lambda_{i}} \qquad \text{s.t.} \quad0\leq \lambda_{i}\leq C，\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>同样的方法求解得到</p>
<script type="math/tex; mode=display">
w^{\ast}=\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}\phi\left( x_{i} \right)}\\
b^{\ast}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}\phi\left( x_{i} \right)^{T}\phi\left( x_{k} \right)}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}K\left( x_{i}, x_{k} \right)}</script><p>最终决策函数为</p>
<script type="math/tex; mode=display">
f\left( x \right) = \text{sign}\left( \sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}K\left( x_{i}, x \right)}  + b^{\ast} \right)</script><p>另外，常用核函数有</p>
<ul>
<li>多项式核函数</li>
<li>高斯核函数</li>
</ul>
<h2 id="SMO-算法"><a href="#SMO-算法" class="headerlink" title="SMO 算法"></a>SMO 算法</h2>]]></content>
  </entry>
  <entry>
    <title>矩阵SVD分解</title>
    <url>/2020/05/15/%E7%9F%A9%E9%98%B5SVD%E5%88%86%E8%A7%A3/</url>
    <content><![CDATA[<p>特征值分解矩阵不能适用于所有矩阵，对于无法进行特征值分解的矩阵，我们使用SVD分解的方法。</p>
<a id="more"></a>
<p>对任意一个 $m\times n$ 矩阵 $A$ ，一定可以进行SVD分解，数学形式为</p>
<script type="math/tex; mode=display">
A_{m\times n}=U_{m\times m}D_{m\times n}V^{T}_{n\times n}</script><p>其中，矩阵 $U$ 的每一个列向量是左奇异向量，对应 $AA^{T}$ 的特征向量，矩阵 $V$ 的每一个列向量（也就是矩阵 $V^{T}$ 的每一个行向量）是右奇异向量，对应 $A^{T}A$  的特征向量。</p>
<p>另外，矩阵 $D$ 有可能是“高瘦型”，也有可能是“矮胖型”，这取决于矩阵 $A$ 的形状，但是不论形状如何，矩阵 $D$ 只有在主对角线上存在非零元素，这些元素是 $AA^{T}$ 的特征值的算数平方根，也是 $A^{T}A$ 的特征值的算数平方根，称为矩阵 $A$ 的奇异值。</p>
<p>Python中的Numpy库有SVD分解方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
<p>首先定义一个2行4列的矩阵，其中元素的值随意。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>u, d, vt = np.linalg.svd(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>u</span><br><span class="line">array([[-<span class="number">0.37616823</span>, -<span class="number">0.92655138</span>],</span><br><span class="line">       [-<span class="number">0.92655138</span>,  <span class="number">0.37616823</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">array([<span class="number">14.22740741</span>,  <span class="number">1.25732984</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vt</span><br><span class="line">array([[-<span class="number">0.35206169</span>, -<span class="number">0.44362578</span>, -<span class="number">0.53518987</span>, -<span class="number">0.62675396</span>],</span><br><span class="line">       [ <span class="number">0.75898127</span>,  <span class="number">0.3212416</span> , -<span class="number">0.11649807</span>, -<span class="number">0.55423774</span>],</span><br><span class="line">       [-<span class="number">0.40008743</span>,  <span class="number">0.25463292</span>,  <span class="number">0.69099646</span>, -<span class="number">0.54554195</span>],</span><br><span class="line">       [-<span class="number">0.37407225</span>,  <span class="number">0.79697056</span>, -<span class="number">0.47172438</span>,  <span class="number">0.04882607</span>]])</span><br></pre></td></tr></table></figure><br>可以看出左右奇异向量所在的矩阵的形状。特别注意，中间的矩阵 $D$ 只保存了主对角线上的非零元素，并且是按照由大到小的顺序排列。</p>
<p>矩阵SVD分解是为了减少大矩阵在计算机中的存储，提高矩阵运算速度。我们需要在数学表达上进一步讨论。</p>
<script type="math/tex; mode=display">
A_{m\times n}=U_{m\times m}D_{m\times n}V^{T}_{n\times n}=\sum_{i=1}^{r}{\lambda_{i}u_{i}v_{i}}</script><p>其中，求和表达式中的 $r$ 是矩阵 $A$ 的非零奇异值个数，也是矩阵 $A$ 的秩， $\lambda$ 是奇异值。求和表达式展开后，我们只保留了矩阵 $D$ 主对角线上的非零奇异值，并按照 $\lambda$ 降序排列每一项，保留系数大的项，舍去系数小的项，实现近似，减少需要存储的值。如果只需要保留前 $k$ 项，则可以写为</p>
<script type="math/tex; mode=display">
A_{m\times n}\approx\sum_{i=1}^{k}{\lambda_{i}u_{i}v_{i}}=U_{m\times k}D_{k\times k}V^{T}_{k\times n}</script><p>在应用中，例如推荐系统中，由于用户和商品数量极其庞大，由用户和商品构成的矩阵也非常庞大，矩阵中每一个元素表达了某个用户对某个商品的评价或喜好。对于一个用户，使用他对每一个商品的评价构成的向量作为描述这个用户的特征，同样对一个商品，我们使用每一用户对它的评价作为这个商品的特征。在使用了SVD分解并近似后，其实是舍弃了小奇异值所对应的“特征”向量（这里的特征向量是描述用户或商品特征的向量），减少了运算量。</p>
<p>在Python中，Numpy的SVD分解方法，返回的奇异值已经降序排序，奇异向量也对应调整过位置，运用矩阵切割就可以实现近似。</p>
<p>矩阵的SVD分解主要运用在利用协同过滤方法实现推荐系统中，充分利用奇异值和奇异向量，对没有直接关联的用户和商品，预测出用户对商品的评价，据此选择是否推荐。</p>
]]></content>
  </entry>
</search>
