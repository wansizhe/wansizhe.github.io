<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>阿里巴巴CVR预估模型：ESMM</title>
    <url>/2020/12/01/Alibaba-ESMM/</url>
    <content><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1804.07931">Entire Space Multi-Task Model: An Eﬀective Approach for Estimating Post-Click Conversion Rate</a></p>
<a id="more"></a>
<h1 id="0x00-CVR与CTR的区别"><a href="#0x00-CVR与CTR的区别" class="headerlink" title="0x00 CVR与CTR的区别"></a>0x00 CVR与CTR的区别</h1><p>CTR是点击率，CVR是转化率，点击率问题非常直接，其含义只有两步：</p>
<ol>
<li>页面曝光</li>
<li>用户点击</li>
</ol>
<p>转化率中涉及的一般是转化为收益的行为，例如淘宝场景中的下单购买。转化率问题在业务逻辑上没有点击率问题那么简单，通常包含三步：</p>
<ol>
<li>页面曝光</li>
<li>用户点击</li>
<li>用户购买</li>
</ol>
<p><img src="/2020/12/01/Alibaba-ESMM/CTR和CVR的样本关系.png" alt="CTR和CVR的样本关系"></p>
<p>转化行为发生前，一定会出现点击行为，这是不可避免的。但是CVR预估问题，最终预估的概率不是“点击且转化”（这是CTCVR问题），而是“点击条件下转化”，这之间的区别和联系可以用下面这个式子表达</p>
<script type="math/tex; mode=display">
P\left(click,conversion\right) = P\left(click\right)P\left(conversion|click\right)</script><p>也就是</p>
<script type="math/tex; mode=display">
P_{CTCVR} = P_{CTR}\cdot P_{CVR}</script><p>所以不能使用全部的样本进行训练，假设样本由特征和标签组成，标签为1，代表成功转化，样本特征如果不做特别注明，那么就无法知道样本是否被点击过，这样训练的模型预测的是CTCVR，而不是CVR。联系实际场景，很多商品你没有购买的直接原因是你没有点击它，CVR预估模型需要预测一个物品假设你点击了，买的概率有多大。</p>
<p>从以上问题中梳理出以下两个重要问题：</p>
<ol>
<li>Sample Selection Bias（SSB）。通常CVR预估问题在模型训练时，提供的样本局限于被点击的记录，其中成功转化的作为正样本，没有转化的作为负样本。但是在预测时，是对所有样本进行预测，而不是对被点击的样本进行预测。样本的选择造成了模型的偏差。</li>
<li>Data Sparsity（DS）。当样本被局限于点击过的数据时，样本量会骤减，远不如CTR问题中曝光的数据量。</li>
</ol>
<h1 id="0x01-模型"><a href="#0x01-模型" class="headerlink" title="0x01 模型"></a>0x01 模型</h1><p><img src="/2020/12/01/Alibaba-ESMM/ESMM.png" alt="ESMM模型结构图"></p>
<p>这个模型体现出了CTCVR、CTR、CVR三者的关系，左边负责预测CVR，右边负责预测CTR，顶部通过相乘预测CTCVR。左侧CVR预估模型不直接参加计算损失函数，损失函数只包含CTR和CTCVR两部分</p>
<script type="math/tex; mode=display">
Loss\left(\theta_{ctr},\theta_{cvr}\right) = 
\sum_{i=1}^{N}{L\left(y_{i},f\left(x_{i};\theta_{ctr}\right)\right)}+
\sum_{i=1}^{N}{L\left(y_{i}\&z_{i},f\left(x_{i};\theta_{ctr}\right)\cdot f\left(x_{i};\theta_{cvr}\right)\right)}</script><p>这样设计，是因为CTR预估任务和CTCVR预估任务可以使用全部的样本，通过“曲线救国”的方式，绕开了CVR预估的样本问题。</p>
<h1 id="0x02-实验"><a href="#0x02-实验" class="headerlink" title="0x02 实验"></a>0x02 实验</h1><p>作者对比的模型如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">结构说明（逐渐复杂）</th>
<th style="text-align:center">训练样本（逐渐扩充）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">BASE</td>
<td style="text-align:center">仅CVR部分</td>
<td style="text-align:center">仅包含点击样本</td>
</tr>
<tr>
<td style="text-align:center">AMAN</td>
<td style="text-align:center">同上</td>
<td style="text-align:center">从未点击样本中采样作为负例加入</td>
</tr>
<tr>
<td style="text-align:center">OVERSAMPLING</td>
<td style="text-align:center">同上</td>
<td style="text-align:center">对正例（点击并转化）过采样</td>
</tr>
<tr>
<td style="text-align:center">UNBIAS</td>
<td style="text-align:center">同上</td>
<td style="text-align:center">加入rejection sampling</td>
</tr>
<tr>
<td style="text-align:center">DIVISION</td>
<td style="text-align:center">分别训练CTR和CTCVR，相除得CVR</td>
<td style="text-align:center">同上</td>
</tr>
<tr>
<td style="text-align:center">ESMM-NS</td>
<td style="text-align:center">ESMM模型，embedding层不共享参数</td>
<td style="text-align:center">同上</td>
</tr>
<tr>
<td style="text-align:center">ESMM</td>
<td style="text-align:center">完整的ESMM模型</td>
<td style="text-align:center">同上</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>多任务学习</tag>
        <tag>推荐</tag>
        <tag>CVR</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/05/13/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Introduction</category>
      </categories>
  </entry>
  <entry>
    <title>my first blog</title>
    <url>/2020/05/13/my-first-blog/</url>
    <content><![CDATA[<h3 id="this-is-my-first-hexo-blog"><a href="#this-is-my-first-hexo-blog" class="headerlink" title="this is my first hexo blog"></a>this is my first hexo blog</h3><a id="more"></a>
<p>test</p>
<script type="math/tex; mode=display">
f(x)=\frac{P(x)}{Q(x)}</script>]]></content>
      <categories>
        <category>Introduction</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机（SVM）数学推导思路</title>
    <url>/2020/06/03/SVM-math/</url>
    <content><![CDATA[<h1 id="相关定义和说明"><a href="#相关定义和说明" class="headerlink" title="相关定义和说明"></a>相关定义和说明</h1><p>数据集定义为： $\left\{ (x_{i}, y_{i}) \right\}_{i=1}^{N}$ ，其中 $x_{i}\in \mathbb{R}^{p}$ ， $y_{i} \in \left\{ -1, +1 \right\}$ </p>
<p>模型定义为： $f\left( x \right) = \text{sign}\left( w^{T} x + b \right)$ ，其中 $w \in \mathbb{R}^{p}$ </p>
<a id="more"></a>
<p>整体知识框架为</p>
<ul>
<li>Hard-Margin</li>
<li>Soft-Margin</li>
</ul>
<p>具体到以上两部分推导的内部，运用到了</p>
<ul>
<li>间隔</li>
<li>对偶</li>
<li>核技巧（选择性使用）</li>
</ul>
<h1 id="Hard-Margin-硬间隔"><a href="#Hard-Margin-硬间隔" class="headerlink" title="Hard-Margin 硬间隔"></a>Hard-Margin 硬间隔</h1><p>设 $\text{margin}\left( w, b \right)$ 表示给定模型参数的情况下，所有样本到分类超平面的最小距离</p>
<script type="math/tex; mode=display">
\text{margin}\left( w, b \right)=\min distance\left( w, b, x_{i} \right)=\min\frac{1}{\left| \left| w \right| \right|}\left| w^{T}x+b \right|=\min\frac{1}{\left| \left| w \right| \right|}y_{i}\left( w^{T}x+b \right)</script><p>目标是将这个值最大化，优化目标为</p>
<script type="math/tex; mode=display">
\max_{w, b}\min_{x_{i}}\ \frac{1}{\left| \left| w \right| \right|}y_{i}\left( w^{T}x+b \right) \qquad \text{s.t.}\quad y_{i}\left( w^{T} x_{i}+b \right)> 0</script><p>令 $y_{i}\left( w^{T} x_{i}+b \right)\geq 1$ ，则优化目标变为</p>
<script type="math/tex; mode=display">
\max_{w, b}\ \frac{1}{\left| \left| w \right| \right|} \qquad \text{s.t.} \quad y_{i}\left( w^{T} x_{i}+b \right)\geq 1</script><p>等价于</p>
<script type="math/tex; mode=display">
\min_{w, b}\frac{1}{2}w^{T}w \qquad \text{s.t.} \quad y_{i}\left( w^{T} x_{i}+b \right)\geq 1</script><p>接下来试图让模型参数不受可行条件约束，该优化问题的拉格朗日函数为</p>
<script type="math/tex; mode=display">
L\left( w, b, \lambda \right)=\frac{1}{2}w^{T}w + \sum_{i=1}^{N}{\lambda_{i}\left[1- y_{i}\left( w^{T}x_{i}+b \right) \right]}</script><p>其中 $\lambda\in \mathbb{R}^p$ 且 $\lambda_{i}\geq 0$ </p>
<p>根据广义拉格朗日函数，优化目标等价于（证明略）</p>
<script type="math/tex; mode=display">
\min_{w, b}\max_{\lambda}\ L\left( w, b, \lambda \right) \qquad \text{s.t.} \quad \lambda_{i}\geq 0</script><p>接下来用求导方法求解 $\min_{w, b}\ L\left( w, b, \lambda \right)$</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial b} = 0\quad \Rightarrow \quad\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>代入拉格朗日函数后继续求导</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w} = 0\quad \Rightarrow \quad w=\sum_{i=1}^{N}{\lambda_{i}y_{i}x_{i}}</script><p>继续带入拉格朗日函数，求得 $\min_{w, b}\ L\left( w, b, \lambda \right)$</p>
<script type="math/tex; mode=display">
L\left( w, b, \lambda \right)=-\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}x_{i}^{T}x_{j}}}+\sum_{i=1}^{N}{\lambda_{i}}</script><p>代入对偶问题的优化目标，求这个最小值关于 $\lambda$ 的最大值，优化目标可以写成（下方通过提出负号，将最大化写为最小化）</p>
<script type="math/tex; mode=display">
\min_{\lambda}\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}x_{i}^{T}x_{j}}}-\sum_{i=1}^{N}{\lambda_{i}} \qquad \text{s.t.} \quad \lambda_{i}\geq 0，\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>求解该问题（可以用SMO算法）可以得到最优解 $\lambda^{\ast} = \left( \lambda_{1}^{\ast},\lambda_{1}^{\ast},…,\lambda_{N}^{\ast} \right)^{T}$ </p>
<p>接下来根据KKT条件求解模型参数的最优解。KKT条件包括三类条件</p>
<ul>
<li>求导为0条件： $\frac{\partial L}{\partial w}=0，\frac{\partial L}{\partial b}=0，\frac{\partial L}{\partial \lambda}=0$ </li>
<li>可行条件： $\lambda_{i}^{\ast}\geq 0，1-y_{i}\left( w^{T}x_{i}+b \right)\leq 0$ </li>
<li>松弛互补条件： $\lambda_{i}^{\ast}\left[ 1-y_{i}\left( w^{T}x_{i}+b \right) \right]=0$ </li>
</ul>
<p>根据KKT条件中的松弛互补条件，求解得</p>
<script type="math/tex; mode=display">
w^{\ast}=\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}}\\
b^{\ast}=y_{k}-w^{\ast T}x_{k}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}x^{k}}</script><p>其中样本 $\left( x_{k}, y_{k} \right)$ 满足 $1-y_{i}\left( w^{T}x_{i}+b \right)= 0，\lambda_{k} &gt;0$ ，这个样本就是支持向量</p>
<p>最终，决策函数为</p>
<script type="math/tex; mode=display">
f\left( x \right) = sign\left( \sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}}x  + b^{\ast} \right)</script><h1 id="Soft-Margin-软间隔"><a href="#Soft-Margin-软间隔" class="headerlink" title="Soft-Margin 软间隔"></a>Soft-Margin 软间隔</h1><p>与Hard-Margin相比，可以允许一点点错误，优化目标可以写为</p>
<script type="math/tex; mode=display">
\min_{w, b}\frac{1}{2}w^{T}w+\text{loss}</script><p>其中的损失函数为Hinge-Loss，即合页损失函数</p>
<script type="math/tex; mode=display">
\text{loss} = \sum_{i=1}^{N}{\max\left\{ 0,\ 1-y_{i}\left( w^{T}x_{i}+b \right)  \right\}}</script><p>令 $\xi_{i} = 1-y_{i}\left( w^{T}x_{i}+b \right)$ ，$\xi_{i}\geq 0$ ，则优化目标等价于</p>
<script type="math/tex; mode=display">
\min_{w, b}\frac{1}{2}w^{T}w+C\sum_{i=1}^{N}{\xi_{i}} \qquad \text{s.t.}\quad y_{i}\left( w^{T}x_{i}+b \right)\geq1-\xi_{i}，\xi_{i}\geq0</script><p>其中，$C$ 为超参数。与Hard-Margin的推导类似，最终优化目标等价于</p>
<script type="math/tex; mode=display">
\min_{\lambda}\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}x_{i}^{T}x_{j}}}-\sum_{i=1}^{N}{\lambda_{i}} \qquad \text{s.t.} \quad0\leq \lambda_{i}\leq C，\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>通过对 $\lambda$ 添加上限，影响了支持向量，影响了最优解。同样方法求得</p>
<script type="math/tex; mode=display">
w^{\ast}=\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}}\\
b^{\ast}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}x_{k}}</script><p>最终决策函数为</p>
<script type="math/tex; mode=display">
f\left( x \right) = \text{sign}\left( \sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}x_{i}^{T}}x  + b^{\ast} \right)</script><h1 id="Kernel-核函数"><a href="#Kernel-核函数" class="headerlink" title="Kernel 核函数"></a>Kernel 核函数</h1><p>将普通SVM中的 $x$ 进行替换， $x\rightarrow\phi\left( x \right)$ ，核函数 $K\left( x, z \right)=\phi\left( x \right)^{T}\phi\left( z \right)$ ，以Soft-Margin为例，优化目标等价于</p>
<script type="math/tex; mode=display">
\min_{\lambda}\frac{1}{2}\sum_{i=1}^{N}{\sum_{j=1}^{N}{\lambda_{i}\lambda_{j}y_{i}y_{j}K\left( x_{i}, x_{k} \right)}}-\sum_{i=1}^{N}{\lambda_{i}} \qquad \text{s.t.} \quad0\leq \lambda_{i}\leq C，\sum_{i=1}^{N}{\lambda_{i}y_{i}}=0</script><p>同样的方法求解得到</p>
<script type="math/tex; mode=display">
w^{\ast}=\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}\phi\left( x_{i} \right)}\\
b^{\ast}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}\phi\left( x_{i} \right)^{T}\phi\left( x_{k} \right)}=y_{k}-\sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}K\left( x_{i}, x_{k} \right)}</script><p>最终决策函数为</p>
<script type="math/tex; mode=display">
f\left( x \right) = \text{sign}\left( \sum_{i=1}^{N}{\lambda_{i}^{\ast}y_{i}K\left( x_{i}, x \right)}  + b^{\ast} \right)</script><p>另外，常用核函数有</p>
<ul>
<li>多项式核函数</li>
<li>高斯核函数</li>
</ul>
<h1 id="SMO-算法"><a href="#SMO-算法" class="headerlink" title="SMO 算法"></a>SMO 算法</h1>]]></content>
      <categories>
        <category>机器学习算法与模型</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵SVD分解</title>
    <url>/2020/05/15/%E7%9F%A9%E9%98%B5SVD%E5%88%86%E8%A7%A3/</url>
    <content><![CDATA[<p>特征值分解矩阵不能适用于所有矩阵，对于无法进行特征值分解的矩阵，我们使用SVD分解的方法。</p>
<a id="more"></a>
<p>对任意一个 $m\times n$ 矩阵 $A$ ，一定可以进行SVD分解，数学形式为</p>
<script type="math/tex; mode=display">
A_{m\times n}=U_{m\times m}D_{m\times n}V^{T}_{n\times n}</script><p>其中，矩阵 $U$ 的每一个列向量是左奇异向量，对应 $AA^{T}$ 的特征向量，矩阵 $V$ 的每一个列向量（也就是矩阵 $V^{T}$ 的每一个行向量）是右奇异向量，对应 $A^{T}A$  的特征向量。</p>
<p>另外，矩阵 $D$ 有可能是“高瘦型”，也有可能是“矮胖型”，这取决于矩阵 $A$ 的形状，但是不论形状如何，矩阵 $D$ 只有在主对角线上存在非零元素，这些元素是 $AA^{T}$ 的特征值的算数平方根，也是 $A^{T}A$ 的特征值的算数平方根，称为矩阵 $A$ 的奇异值。</p>
<p>Python中的Numpy库有SVD分解方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
<p>首先定义一个2行4列的矩阵，其中元素的值随意。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>u, d, vt = np.linalg.svd(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>u</span><br><span class="line">array([[-<span class="number">0.37616823</span>, -<span class="number">0.92655138</span>],</span><br><span class="line">       [-<span class="number">0.92655138</span>,  <span class="number">0.37616823</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">array([<span class="number">14.22740741</span>,  <span class="number">1.25732984</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vt</span><br><span class="line">array([[-<span class="number">0.35206169</span>, -<span class="number">0.44362578</span>, -<span class="number">0.53518987</span>, -<span class="number">0.62675396</span>],</span><br><span class="line">       [ <span class="number">0.75898127</span>,  <span class="number">0.3212416</span> , -<span class="number">0.11649807</span>, -<span class="number">0.55423774</span>],</span><br><span class="line">       [-<span class="number">0.40008743</span>,  <span class="number">0.25463292</span>,  <span class="number">0.69099646</span>, -<span class="number">0.54554195</span>],</span><br><span class="line">       [-<span class="number">0.37407225</span>,  <span class="number">0.79697056</span>, -<span class="number">0.47172438</span>,  <span class="number">0.04882607</span>]])</span><br></pre></td></tr></table></figure><br>可以看出左右奇异向量所在的矩阵的形状。特别注意，中间的矩阵 $D$ 只保存了主对角线上的非零元素，并且是按照由大到小的顺序排列。</p>
<p>矩阵SVD分解是为了减少大矩阵在计算机中的存储，提高矩阵运算速度。我们需要在数学表达上进一步讨论。</p>
<script type="math/tex; mode=display">
A_{m\times n}=U_{m\times m}D_{m\times n}V^{T}_{n\times n}=\sum_{i=1}^{r}{\lambda_{i}u_{i}v_{i}}</script><p>其中，求和表达式中的 $r$ 是矩阵 $A$ 的非零奇异值个数，也是矩阵 $A$ 的秩， $\lambda$ 是奇异值。求和表达式展开后，我们只保留了矩阵 $D$ 主对角线上的非零奇异值，并按照 $\lambda$ 降序排列每一项，保留系数大的项，舍去系数小的项，实现近似，减少需要存储的值。如果只需要保留前 $k$ 项，则可以写为</p>
<script type="math/tex; mode=display">
A_{m\times n}\approx\sum_{i=1}^{k}{\lambda_{i}u_{i}v_{i}}=U_{m\times k}D_{k\times k}V^{T}_{k\times n}</script><p>在应用中，例如推荐系统中，由于用户和商品数量极其庞大，由用户和商品构成的矩阵也非常庞大，矩阵中每一个元素表达了某个用户对某个商品的评价或喜好。对于一个用户，使用他对每一个商品的评价构成的向量作为描述这个用户的特征，同样对一个商品，我们使用每一用户对它的评价作为这个商品的特征。在使用了SVD分解并近似后，其实是舍弃了小奇异值所对应的“特征”向量（这里的特征向量是描述用户或商品特征的向量），减少了运算量。</p>
<p>在Python中，Numpy的SVD分解方法，返回的奇异值已经降序排序，奇异向量也对应调整过位置，运用矩阵切割就可以实现近似。</p>
<p>矩阵的SVD分解主要运用在利用协同过滤方法实现推荐系统中，充分利用奇异值和奇异向量，对没有直接关联的用户和商品，预测出用户对商品的评价，据此选择是否推荐。</p>
]]></content>
      <categories>
        <category>机器学习算法与模型</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里巴巴QAC搜索词补全模型：M&lt;sup&gt;2&lt;/sup&gt;A</title>
    <url>/2020/12/09/Alibaba-M2A/</url>
    <content><![CDATA[<p>论文地址：<a href="https://dl.acm.org/doi/10.1145/3394486.3403350">Learning to Generate Personalized Query Auto-Completions via a Multi-View Multi-Task Attentive Approach</a></p>
<a id="more"></a>
<h1 id="0x00-问题背景"><a href="#0x00-问题背景" class="headerlink" title="0x00 问题背景"></a>0x00 问题背景</h1><p>QAC，Query Auto-Completion</p>
<p><strong>三个要求</strong><br>一个好的QAC系统需要达到以下三个要求：</p>
<ol>
<li>前缀词要包含于每一个推荐的查询词中；</li>
<li>在线服务要能够实现查询词自动填充，无论前缀词是否合理；</li>
<li>最关键的是，所有提供的候选查询词，都应该和用户当前的搜索意图相关。</li>
</ol>
<p><strong>两步模型</strong><br>大多数在线搜索服务使用两步模型：匹配和排序。</p>
<ol>
<li>匹配（Match），和召回含义相似，从词库中召回一批完整的查询词，其中包含很多从搜索日志中提取出的热门词；</li>
<li>排序（Rank），就需要对召回的候选词进行排序，排序应该依据搜索词和用户搜索意图的符合程度；</li>
</ol>
<p>最后，根据实际的展示框大小，列出排名靠前的搜索词</p>
<p><strong>两个问题</strong><br>当前QAC系统存在两个问题：缺乏个性化和未登录搜索词。</p>
<ol>
<li>缺乏个性化（Weak Personalization），由于趋向于将热门词排得很靠前，导致有些词甚至和用户搜索意图关系不大，这种情况在前缀词非常短的时候经常出现。为了解决这一问题，一般会在learning to rank框架中考虑多种特征，例如搜索上下文、时间、搜索人的角色、重构行为和在线趋势。但是还没有考虑给用户多种形式的历史行为进行建模（例如搜索历史、浏览历史）。</li>
<li>未登录搜索词（Unseen Queries），如果用户输入的搜索词没有在词库中出现过，或者更进一步，用户输入的词无法召回任何一个候选词。一般会用RNN等语言模型为输入的前缀词自动生成完整的搜索词，这样的解决方案会出现严重的缺乏个性化的问题。</li>
</ol>
<h1 id="0x01-模型"><a href="#0x01-模型" class="headerlink" title="0x01 模型"></a>0x01 模型</h1><p>文章提出新模型，一个重要目标从用户历史行为中建模表示出用户近期的搜索意图。但是用户历史行为存在多样性和动态性，多样性指的是用户历史行为中包含有很多个人兴趣，但不是每一个都和当前的搜索意图相关；动态性指的是用户的行为是动态变化的，其中可能存在复杂的联系。</p>
<p>文章提出新模型M<sup>2</sup>A（Multi-view Multi-task Attentive）从两个角度提取用户历史行为中的有用信息：搜索词历史和浏览物品历史。用一个基于Transformer的层次编码器，先提取历史行为中每一个具体的特征，再提取行为序列的特征；然后在前缀词和多角度历史行为信息之间构建注意力机制，选择最相关和最有用的信息。模型的下游任务有两个：点击率预估和搜索词生成，两个任务同时进行。</p>
<p><img src="/2020/12/09/Alibaba-M2A/M2A模型结构.png" alt="M&lt;sup&gt;2&lt;/sup&gt;A模型结构图"></p>
<h2 id="Prefix-Transformer-Encoder"><a href="#Prefix-Transformer-Encoder" class="headerlink" title="Prefix Transformer Encoder"></a>Prefix Transformer Encoder</h2><p>结构图中间部分。</p>
<p>前缀词（Prefix）送入一个Transformer编码器中进行编码，这主要是为后续注意力机制做铺垫。</p>
<h2 id="Multi-View-Attentive-Hierarchical-Encoding"><a href="#Multi-View-Attentive-Hierarchical-Encoding" class="headerlink" title="Multi-View Attentive Hierarchical Encoding"></a>Multi-View Attentive Hierarchical Encoding</h2><p>结构图左边部分。</p>
<p>多角度、多层次、带注意力的对历史行为的编码。这个名字就包含了这个编码中的方法细节</p>
<ul>
<li>Hierarchical 指的是行为级编码（Behavior-Level）和上下文级编码（Context-Level）；</li>
<li>Attentive 指的是模型中用到了注意力机制（Multi-Head Context Attention）；</li>
<li>Multi-View 指的是搜索词历史和浏览物品历史（Multi-View Information Selection）。</li>
</ul>
<p>接下来按照顺序介绍这三部分</p>
<h3 id="Behavior-Level-Transformer-Encoder-with-Multi-Head-Pooling"><a href="#Behavior-Level-Transformer-Encoder-with-Multi-Head-Pooling" class="headerlink" title="Behavior-Level Transformer Encoder with Multi-Head Pooling"></a>Behavior-Level Transformer Encoder with Multi-Head Pooling</h3><p>行为级编码。</p>
<p><strong>Transformer and Embedding Layers</strong><br>常规的Transformer，在Embedding Layer有加上Positional Enocding</p>
<p><strong>Multi-Head Pooling</strong><br>为了从长度不固定的行为特征中得到固定长度的特征表示，使用到了这个技术，本质还是注意力机制，多每一个head的输出进行加权求和并Layer Normalization</p>
<h3 id="Context-Level-Transformer-Encoder："><a href="#Context-Level-Transformer-Encoder：" class="headerlink" title="Context-Level Transformer Encoder："></a>Context-Level Transformer Encoder：</h3><p>上下文级编码。</p>
<h3 id="Multi-Head-Context-Attention"><a href="#Multi-Head-Context-Attention" class="headerlink" title="Multi-Head Context Attention"></a>Multi-Head Context Attention</h3><p>上下文多头注意力。</p>
<p>利用了前面Prefix的编码结果，对用户多个历史行为构建注意力机制，加权求和得到每一个角度的历史行为的上下文级特征，后续通过拼接（Combination）送入下游任务。</p>
<h3 id="Multi-View-Information-Selection"><a href="#Multi-View-Information-Selection" class="headerlink" title="Multi-View Information Selection"></a>Multi-View Information Selection</h3><p>多角度信息选择。</p>
<p>由于用户会点击和自己搜索意图相近的物品，所以浏览历史可以作为搜索历史的补充，利用这两个角度的历史行为可以得到更综合的信息。</p>
<h2 id="Multi-Task-Learning"><a href="#Multi-Task-Learning" class="headerlink" title="Multi-Task Learning"></a>Multi-Task Learning</h2><p>结构图右边部分。</p>
<p>下游是多任务学习，一个任务是根据前缀词（Prefix）生成完整的搜索词（Query），另一个是搜索词的点击率预估。在实际运用时，CTR预估任务可以给出一个推荐搜索词的list，Transformer Decoder生成的词最为补充放置在list的靠前的位置。</p>
<p><strong>Query Generation</strong><br>这是一个Transformer Decoder，使用Transformer的解码器直接生成候选词。这是端到端（end-to-end）的流程，需要最大化生成词出现的概率，用MLE（极大似然估计，Maximum Likelihood Estimation）的方法进行优化。训练时，可以使用用户输入的前缀和点击的搜索词构成训练数据，但是对于未登录词，无法采集样本，可以使用老方法，从完整的搜索词中切分出前缀，构造数据。</p>
<p><strong>CTR Prediction</strong><br>数据的label：1代表点击、0代表没有点击。预测出点击概率，按照概率进行排序。给定用户意图特征和一个候选词特征，一起送入全连接层，用MLP（多层感知机，Multi-Layer Perceptron）输出点击概率。实际运用时，正样本好采集，负样本需要构造，最后最小化交叉熵损失。</p>
<h1 id="0x02-实验"><a href="#0x02-实验" class="headerlink" title="0x02 实验"></a>0x02 实验</h1><p>文章提出了一个新的数据集：TaobaoQAC，包含了丰富的“前缀-搜索词”（Prefix-Query）点击行为。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>多任务学习</tag>
        <tag>搜索</tag>
        <tag>QAC</tag>
      </tags>
  </entry>
  <entry>
    <title>生成高质量查询推荐词</title>
    <url>/2020/12/13/Generate-QS/</url>
    <content><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1802.07997">Generating High-Quality Query Suggestion Candidates for Task-Based Search</a></p>
<a id="more"></a>
<h1 id="0x00-概述"><a href="#0x00-概述" class="headerlink" title="0x00 概述"></a>0x00 概述</h1><p>查询词推荐有两种形式，查询词补齐（query completions）和查询词优化（query refinements）。查询推荐包括两个步骤：先做推荐词的生成（suggestion generation），然后进行推荐词排序（suggestion ranking），这篇论文主要讲第一步，目标是生成足够多的高质量的候选词，并且保证多样性，以供之后的排序。</p>
<p>作者后文讲了三种方法，通过实验发现三种方法都可以产生了独特的候选词，所以最好是将它们合并起来交给排序模型。</p>
<h1 id="0x01-方法"><a href="#0x01-方法" class="headerlink" title="0x01 方法"></a>0x01 方法</h1><p>假设起始的搜索词为 $q_{0}$ ，给出的推荐词是 $q$ ，需要最大化 $P\left(q|q_{0}\right)$</p>
<h2 id="Popular-Suffix-Model"><a href="#Popular-Suffix-Model" class="headerlink" title="Popular Suffix Model"></a>Popular Suffix Model</h2><p>对一个输入的查询词，从日志信息中获取出现次数最多的后缀词$s$，通过拼接产生推荐词。</p>
<script type="math/tex; mode=display">
P\left(q|q_{0}\right)=pop\left(s\right)</script><p>其中$pop\left(s\right)$和在日志中出现的频率相关</p>
<h2 id="Neural-Language-Model"><a href="#Neural-Language-Model" class="headerlink" title="Neural Language Model"></a>Neural Language Model</h2><p>以词为单位将输入的查询词扩展成推荐词，使用的是神经语言模型。</p>
<script type="math/tex; mode=display">
q_{0}=\left(c_{1},c_{2},...,c_{n}\right) \\
s=\left(c_{n+1},c_{n+2},...,c_{m}\right) \\
q=q_{0}\oplus s=\left(c_{1},c_{2},...,c_{n},c_{n+1},c_{n+2},...,c_{m}\right)</script><p>一般使用循环神经网络，例如LSTM、GRU，来根据已有的序列，预测出后续的序列。</p>
<p>作者的实验设置：hidden_size为512，embedding_layer参数直接从Bing预训练好的迁移过来，beam_search_width设置为30。</p>
<h2 id="Sequence-to-Sequence-Model"><a href="#Sequence-to-Sequence-Model" class="headerlink" title="Sequence to Sequence Model"></a>Sequence to Sequence Model</h2><p>这个模型包含了语义理解，可以做到refinement，前两种方法只能做到completion将输入的查询词”翻译“成新的搜索词来推荐给用户</p>
<script type="math/tex; mode=display">
q_{0} = \left(w_{1},w_{2},...,w_{n}\right) \\
q=\left(w'_{1},w'_{2},...,w'_{m}\right)</script><p>Seq2Seq模型的Encoder和Decoder可以使用LSTM或者GRU。</p>
<p>作者在实验中使用的是GRU，编码器hidden_size为100，解码器hidden_size为200，使用Adam优化器，起始learning_rate为0.0001，dropout概率为0.5，beam_search_width设置为100。</p>
<h1 id="0x02-数据来源"><a href="#0x02-数据来源" class="headerlink" title="0x02 数据来源"></a>0x02 数据来源</h1><p>三种数据来源</p>
<ul>
<li>AOL</li>
<li>KnoweHow</li>
<li>WikiAnswers</li>
</ul>
<p>两种数据组织形式：</p>
<ul>
<li>短文本集合，用于前两种方法，学习complete</li>
<li>同session内的文本对（question-suggestion），用于最后一种方法，学习translate</li>
</ul>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>搜索</tag>
        <tag>QS</tag>
        <tag>LSTM</tag>
        <tag>Seq2Seq</tag>
      </tags>
  </entry>
  <entry>
    <title>期望最大算法（EM）数学推导思路</title>
    <url>/2020/06/24/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h1><p>已知：</p>
<ul>
<li>观测变量 $X$</li>
<li>人为定义或自然存在的隐变量 $Z$</li>
<li>联合概率分布 $P\left( X,Z | \theta \right)$ </li>
<li>条件概率分布 $P\left( Z | X, \theta \right)$</li>
</ul>
<a id="more"></a>
<p>求解：</p>
<ul>
<li>模型参数 $\theta$</li>
</ul>
<p>根本思想：</p>
<ul>
<li>用极大似然估计求解， $\theta_{MLE}= \mathop{\arg\max}_{\theta}\ \log P\left( X|\theta \right)$ ，充分利用联合概率、条件概率和边缘概率的关系来帮助运算</li>
</ul>
<h1 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h1><p>输入： $X$ ， $Z$ ，$P\left( X,Z | \theta \right)，P\left( Z | X, \theta \right)$</p>
<p>输出： $\theta$</p>
<p>算法：</p>
<ol>
<li>初始化参数： <script type="math/tex; mode=display">
\theta^{\left( 0 \right)}</script></li>
<li>E-step，写出Q函数： <script type="math/tex; mode=display">
Q\left( \theta, \theta^{\left( t \right)} \right)=E_{Z|X,\theta^{\left( t \right)}}\left[ \log P\left( X,Z|\theta \right) \right]=\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right)\log P\left( X,Z|\theta \right) \mathrm{d}Z</script></li>
<li>M-step，最大化Q函数： <script type="math/tex; mode=display">
\theta^{\left( t+1 \right)}=\mathop{\arg\max}_{\theta}\ Q\left( \theta, \theta^{\left( t \right)} \right)</script></li>
</ol>
<h1 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h1><p>推导目的： $\mathop{\arg\max}_{\theta}\ \log P\left( X|\theta \right)\quad\Rightarrow\quad \mathop{\arg\max}_{\theta}\ Q\left( \theta, \theta^{\left( t \right)} \right)$ </p>
<h3 id="方法一（运用KL散度）"><a href="#方法一（运用KL散度）" class="headerlink" title="方法一（运用KL散度）"></a>方法一（运用KL散度）</h3><p>根据联合概率和边缘概率的关系，得到</p>
<script type="math/tex; mode=display">
\log P\left( X | \theta \right) = \log P\left( X,Z | \theta \right)-\log P\left( Z | X,\theta \right)</script><p>引入 Z 的概率分布函数 $q\left( Z \right)$ ，得到</p>
<script type="math/tex; mode=display">
\log P\left( X | \theta \right) = \log \frac{P\left( X,Z | \theta \right)}{q\left( Z \right)}-\log \frac{P\left( Z | X,\theta \right)}{q\left( Z \right)}</script><p>等式左边可以写成</p>
<script type="math/tex; mode=display">
\log P\left( X | \theta \right) = \log P\left( X | \theta \right) \int_{Z}q\left( Z \right) \mathrm{d}Z=\int_{Z}q\left( Z \right) \log P\left( X | \theta \right) \mathrm{d}Z</script><p>同理，右边也可以写成</p>
<script type="math/tex; mode=display">
\log \frac{P\left( X,Z | \theta \right)}{q\left( Z \right)}-\log \frac{P\left( Z | X,\theta \right)}{q\left( Z \right)}=\int_{Z}q\left( Z \right) \log \frac{P\left( X,Z | \theta \right)}{q\left( Z \right)} \mathrm{d}Z-\int_{Z}q\left( Z \right) \log \frac{P\left( Z | X,\theta \right)}{q\left( Z \right)} \mathrm{d}Z</script><p>定义Evidence Lower Bound（ELBO）</p>
<script type="math/tex; mode=display">
{\rm ELBO}=\int_{Z}q\left( Z \right) \log \frac{P\left( X,Z | \theta \right)}{q\left( Z \right)} \mathrm{d}Z</script><blockquote>
<p>KL散度定义： ${\rm KL}\left( P || Q \right)=\int_{x}P\left( x \right)\log \frac{P\left( x \right)}{Q\left( x \right)} \mathrm{d}x\geq 0$ ，当且仅当 $P\left( x \right)=Q\left( x \right)$ 时等号成立。</p>
</blockquote>
<p>根据KL散度定义，可以得出</p>
<script type="math/tex; mode=display">
\int_{Z}q\left( Z \right) \log \frac{P\left( Z | X,\theta \right)}{q\left( Z \right)} \mathrm{d}Z={\rm KL}\left[ q\left( Z \right) || P\left( Z | X,\theta \right) \right]\geq0</script><p>当且仅当 $q\left( Z \right) = P\left( Z | X,\theta \right)$ 时等号成立。所以，根据以上可得</p>
<script type="math/tex; mode=display">
\log P\left( X | \theta \right) ={\rm ELBO}+ {\rm KL}\left[ q\left( Z \right) || P\left( Z | X,\theta \right) \right]\geq {\rm ELBO}</script><p>令 $q\left( Z \right) = P\left( Z | X,\theta^{\left( t \right)} \right)$ ，则等号成立，所以</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\hat{\theta}&=\mathop{\arg\max}_{\theta}\ {\rm ELBO}\\ 
&=\mathop{\arg\max}_{\theta}\ \log P\left( X|\theta \right)\\ 
&=\mathop{\arg\max}_{\theta}\ \int_{Z}q\left( Z \right) \log \frac{P\left( X,Z | \theta \right)}{q\left( Z \right)} \mathrm{d}Z \\ 
&=\mathop{\arg\max}_{\theta}\ \int_{Z}P\left( Z | X,\theta^{\left( t \right)} \right) \log \frac{P\left( X,Z | \theta \right)}{P\left( Z | X,\theta^{\left( t \right)} \right)} \mathrm{d}Z \\ 
&=\mathop{\arg\max}_{\theta}\ \int_{Z}P\left( Z | X,\theta^{\left( t \right)} \right) \left[ \log P\left( X,Z | \theta \right) - \log P\left( Z | X,\theta^{\left( t \right)} \right) \right] \mathrm{d}Z \\ 
\end{aligned}</script><p>至此，可以看出 $\log P\left( Z | X,\theta^{\left( t \right)} \right)$ 与 $\theta$ 无关， $\theta^{\left( t \right)}$ 是上一轮迭代求出的结果，对当前来说是常数，所以</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\hat{\theta}&=\mathop{\arg\max}_{\theta}\ \int_{Z}P\left( Z | X,\theta^{\left( t \right)} \right) \left[ \log P\left( X,Z | \theta \right) - \log P\left( Z | X,\theta^{\left( t \right)} \right) \right] \mathrm{d}Z \\ 
&=\mathop{\arg\max}_{\theta}\ \int_{Z}P\left( Z | X,\theta^{\left( t \right)} \right) \log P\left( X,Z | \theta \right) \mathrm{d}Z \\ 
&=\mathop{\arg\max}_{\theta}\ Q\left( \theta, \theta^{\left( t \right)} \right) \\ 
\end{aligned}</script><h3 id="方法二（运用Jensen不等式）"><a href="#方法二（运用Jensen不等式）" class="headerlink" title="方法二（运用Jensen不等式）"></a>方法二（运用Jensen不等式）</h3><blockquote>
<p>关于Jensen不等式的说明：设 $f\left( x \right)$ 为凸函数， $a\leq c\leq b$ ，存在 $t\in\left[ 0,1 \right]$ 使得 $c=ta+\left( 1-t \right)b$ ，那么有 $f\left( c \right)=f\left( ta+\left( 1-t \right)b \right)\geq tf\left( a \right)+\left( 1-t \right)f\left( b \right)$ ，由此可以推广出 $f\left( E\left[ x \right] \right)\geq E\left[ f\left( x \right) \right]$</p>
</blockquote>
<p>根据联合概率和边缘概率的关系可得</p>
<script type="math/tex; mode=display">
\log P\left( X | \theta \right) =\log \int_{Z}P\left( X,Z| \theta \right) \mathrm{d}Z</script><p>引入 Z 的概率分布函数 $q\left( Z \right)$ ，得到</p>
<script type="math/tex; mode=display">
\log P\left( X | \theta \right) =\log \int_{Z}\frac{P\left( X,Z| \theta \right)}{q\left( Z \right)}q\left( Z \right) \mathrm{d}Z = \log E_{q\left( Z \right)}\left[ \frac{P\left( X,Z| \theta \right)}{q\left( Z \right)} \right]</script><p>根据Jensen不等式，得到</p>
<script type="math/tex; mode=display">
\log P\left( X | \theta \right) = \log E_{q\left( Z \right)}\left[ \frac{P\left( X,Z| \theta \right)}{q\left( Z \right)} \right]\geq E_{q\left( Z \right)}\left[\log\frac{P\left( X,Z| \theta \right)}{q\left( Z \right)} \right]</script><p>当且仅当 $\frac{P\left( X,Z| \theta \right)}{q\left( Z \right)}$ 为常数时取等号。</p>
<p>设 $\frac{P\left( X,Z| \theta \right)}{q\left( Z \right)}=c$ ，则 $q\left( Z \right)=\frac{1}{c}P\left( X,Z| \theta \right)$ ，又因为</p>
<script type="math/tex; mode=display">
1=\int_{Z}q\left( Z \right)\mathrm{d}Z=\int_{Z}\frac{1}{c}P\left( X,Z| \theta \right)\mathrm{d}Z=\frac{1}{c}P\left( X| \theta \right)</script><p>所以就有 $P\left( X| \theta \right)=c$ ，进而就有</p>
<script type="math/tex; mode=display">
q\left( Z \right)=\frac{P\left( X,Z| \theta \right)}{c}=\frac{P\left( X,Z| \theta \right)}{P\left( X| \theta \right)}=P\left( Z|X,\theta \right)</script><p>将前面Jensen不等式的结果展开，得到</p>
<script type="math/tex; mode=display">
E_{q\left( Z \right)}\left[\log\frac{P\left( X,Z| \theta \right)}{q\left( Z \right)} \right]=\int_{Z}q\left( Z \right) \log \frac{P\left( X,Z | \theta \right)}{q\left( Z \right)} \mathrm{d}Z={\rm ELBO}</script><p>令 $q\left( Z \right)=P\left( Z|X,\theta^{\left( t \right)} \right)$ ，代入后得到 </p>
<script type="math/tex; mode=display">
\begin{aligned} 
{\rm ELBO}&=\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \log \frac{P\left( X,Z | \theta \right)}{P\left( Z|X,\theta^{\left( t \right)} \right)} \mathrm{d}Z\\ 
&=\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \left[\log P\left( X,Z | \theta \right)-\log P\left( Z|X,\theta^{\left( t \right)} \right)\right] \mathrm{d}Z \end{aligned}</script><p>因为 $\theta^{\left( t \right)}$ 是上一轮迭代求出的结果，对当前来说是常数，所以 $\log P\left( Z | X,\theta^{\left( t \right)} \right)$ 与 $\theta$ 无关，可以略去，因此得到</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\hat{\theta}&=\mathop{\arg\max}_{\theta}\ \log P\left( X|\theta \right)\\ 
&=\mathop{\arg\max}_{\theta}\ {\rm ELBO} \\ 
&=\mathop{\arg\max}_{\theta}\ \int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \left[\log P\left( X,Z | \theta \right)-\log P\left( Z|X,\theta^{\left( t \right)} \right)\right] \mathrm{d}Z\\ 
&=\mathop{\arg\max}_{\theta}\ \int_{Z}P\left( Z | X,\theta^{\left( t \right)} \right) \log P\left( X,Z | \theta \right) \mathrm{d}Z\\ 
&= \mathop{\arg\max}_{\theta}\ Q\left( \theta, \theta^{\left( t \right)} \right) 
\end{aligned}</script><h1 id="收敛性证明"><a href="#收敛性证明" class="headerlink" title="收敛性证明"></a>收敛性证明</h1><p>这里就是要证明EM算法一定可以达到优化参数的目的。</p>
<p>优化目标是 $\hat{\theta}=\mathop{\arg\max}_{\theta}\ \log P\left( X|\theta \right)$ ，采用EM迭代算法，需要保证 $\theta^{\left( t+1 \right)}$ 比 $\theta^{\left( t \right)}$ 更优，也就是 $\log P\left( X| \theta^{\left( t+1 \right)}\right)\geq \log P\left( X| \theta^{\left( t \right)}\right)$ 。因此需要重点研究 $\log P\left( X|\theta \right)$ 的特点。</p>
<script type="math/tex; mode=display">
\log P\left( X|\theta \right)=\log \frac{P\left( X,Z|\theta \right)}{P\left( Z|X,\theta \right)} = \log P\left( X,Z|\theta \right)-\log P\left( Z|X,\theta \right)</script><p>左边：</p>
<script type="math/tex; mode=display">
\log P\left( X|\theta \right)=\log P\left( X|\theta \right)\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \mathrm{d}Z = \int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \log P\left( X|\theta \right) \mathrm{d}Z</script><p>同理，右边：</p>
<script type="math/tex; mode=display">
\log P\left( X,Z|\theta \right)-\log P\left( Z|X,\theta \right)=\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \log P\left( X,Z|\theta \right) \mathrm{d}Z-\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \log P\left( Z|X,\theta \right) \mathrm{d}Z</script><p>令 $H\left( \theta , \theta^{\left( t \right)} \right)=\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \log P\left( Z|X,\theta \right) \mathrm{d}Z$ ，得到</p>
<script type="math/tex; mode=display">
\log P\left( X|\theta \right)=Q\left( \theta , \theta^{\left( t \right)} \right)-H\left( \theta , \theta^{\left( t \right)} \right)</script><p>若要 $\log P\left( X| \theta^{\left( t+1 \right)}\right)\geq \log P\left( X| \theta^{\left( t \right)}\right)$ ，就需要满足 $Q\left(\theta^{\left( t+1 \right)}, \theta^{\left( t \right)} \right)\geq Q\left(\theta^{\left( t \right)}, \theta^{\left( t \right)} \right)$ 且 $H\left(\theta^{\left( t+1 \right)}, \theta^{\left( t \right)} \right)\leq H\left(\theta^{\left( t \right)}, \theta^{\left( t \right)} \right)$ ，前者天然成立，后者需要证明</p>
<script type="math/tex; mode=display">
H\left(\theta^{\left( t+1 \right)}, \theta^{\left( t \right)} \right)- H\left(\theta^{\left( t \right)}, \theta^{\left( t \right)} \right) = \int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right)\log \frac{P\left( Z|X,\theta^{\left( t+1 \right)} \right)}{P\left( Z|X,\theta^{\left( t \right)} \right)}\mathrm{d}Z</script><p>根据KL散度可以得到</p>
<script type="math/tex; mode=display">
\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right)\log \frac{P\left( Z|X,\theta^{\left( t+1 \right)} \right)}{P\left( Z|X,\theta^{\left( t \right)} \right)}\mathrm{d}Z=-{\rm KL}\left[ P\left( Z|X,\theta^{\left( t \right)} \right)||P\left( Z|X,\theta^{\left( t+1 \right)} \right) \right]\leq 0</script><p>因此得到</p>
<script type="math/tex; mode=display">
H\left(\theta^{\left( t+1 \right)}, \theta^{\left( t \right)} \right)- H\left(\theta^{\left( t \right)}, \theta^{\left( t \right)} \right)\leq0</script><p>或者根据Jensen不等式</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right)\log \frac{P\left( Z|X,\theta^{\left( t+1 \right)} \right)}{P\left( Z|X,\theta^{\left( t \right)} \right)}\mathrm{d}Z&=E_{Z|X,\theta^{\left( t \right)}}\left[ \log \frac{P\left( Z|X,\theta^{\left( t+1 \right)} \right)}{P\left( Z|X,\theta^{\left( t \right)} \right)} \right]\\ 
&\leq\log E_{Z|X,\theta^{\left( t \right)}}\left[ \frac{P\left( Z|X,\theta^{\left( t+1 \right)} \right)}{P\left( Z|X,\theta^{\left( t \right)} \right)} \right]\\ 
&=\log \int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right) \frac{P\left( Z|X,\theta^{\left( t+1 \right)} \right)}{P\left( Z|X,\theta^{\left( t \right)} \right)}\mathrm{d}Z\\ 
&=\log \int_{Z}P\left( Z|X,\theta^{\left( t+1 \right)} \right)\mathrm{d}Z\\ 
&=\log 1 \\ 
&= 0 
\end{aligned}</script><p>因此得到</p>
<script type="math/tex; mode=display">
H\left(\theta^{\left( t+1 \right)}, \theta^{\left( t \right)} \right)- H\left(\theta^{\left( t \right)}, \theta^{\left( t \right)} \right)\leq0</script><p>以上两种方法都可以证明 $H\left(\theta^{\left( t+1 \right)}, \theta^{\left( t \right)} \right)\leq H\left(\theta^{\left( t \right)}, \theta^{\left( t \right)} \right)$ </p>
]]></content>
      <categories>
        <category>机器学习算法与模型</category>
      </categories>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title>高斯混合模型（GMM）数学推导思路</title>
    <url>/2020/07/01/GMM%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88/</url>
    <content><![CDATA[<h1 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h1><p>已知，在数轴上按照某种概率分布采样，得到了如下图横轴上的红点。依据这些采样点反推出具体的概率分布，这就是我们想要做的事。</p>
<a id="more"></a>
<p><img src="/2020/07/01/GMM%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88/一维GMM.png" alt="一维高斯混合模型示例"></p>
<p>概率分布不能凭空猜测，我们假设所要求的概率分布是由多个高斯分布混合而成的，所谓“混合”，可以理解成“加权平均”，数学上可以写成</p>
<script type="math/tex; mode=display">
P\left( x \right)=\sum_{k=1}^{K}{\alpha_{k}\cdot\mathcal N\left( x| \mu_{k},\Sigma_{k} \right)}</script><p>其中，为简便，用 $\mathcal{N}$ 表示高斯分布的表达式， $\mu_{k}$ 表示第 $k$ 个高斯分布的均值向量， $\Sigma_{k}$ 表示第 $k$ 个高斯分布的协方差矩阵， $\alpha_{k}$ 表示第 $k$ 个高斯分布的权重系数，这个系数还满足</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{K}{\alpha_{k}}=1</script><p>有了这种几何理解，再看已知的采样点，可以看到明显的两个密集区域，说明这两个密集区域是概率较大的位置，很有可能是高斯分布的峰值位置，因此可以大致画出两个高斯分布的曲线（如上图蓝线和黄线）。</p>
<p>类似的，也可以想象出一个二维平面中的采样点的分布，由此想像出二维高斯分布的叠加。如下图所示，二维高斯分布用“等高线”的形式大致画出。</p>
<p><img src="/2020/07/01/GMM%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88/二维GMM.png" alt="二维高斯混合模型示例"></p>
<p>以上，是对GMM的几何理解。但是高斯分布的维度限制这种几何角度的想象，更进一步理解，需要从概率生成模型的角度理解。高斯混合模型是一种较简单的概率图模型，也是一种简单的生成模型。</p>
<p><img src="/2020/07/01/GMM%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88/GMM概率生成.png" alt="GMM概率生成模型示意图"></p>
<p>我们假设采样点是这样得到的</p>
<ol>
<li>按照一定的概率分布，选取一个高斯分布</li>
<li>按照选出的高斯分布来采样<br>我们假设已经有了 $K$ 个高斯分布， 根据隐变量 $Z$ 的取值来选择其中一个高斯分布来采样。$P\left( z \right)$ 是一种离散的概率分布<script type="math/tex; mode=display">
P\left( Z=k \right)=p_{k}</script>显然 $\sum_{k=1}^{K}{p_{k}}=1$ ，接下来计算 $P\left( x \right)$ ，需要充分利用联合概率和边缘概率的关系<script type="math/tex; mode=display">
\begin{aligned} 
P\left( x \right)&=\sum_{Z}^{}{P\left( x,Z \right)}\\ 
&=\sum_{k=1}^{K}{P\left( x,Z=k \right)}\\ 
&=\sum_{k=1}^{K}{P\left( x|Z=k \right)P\left( Z=k \right)}\\ 
&=\sum_{k=1}^{K}{p_{k}\cdot \mathcal N\left( x | \mu_{k},\Sigma_{k} \right)} 
\end{aligned}</script>这样，模型的定义就更明确了，我们称</li>
</ol>
<ul>
<li>$X=\left\{ x_{1},x_{2},…,x_{N} \right\}$ 为观测数据，</li>
<li>$\left( X,Z \right)=\left\{ \left( x_{1}, Z_{1} \right),\left( x_{2}, Z_{2} \right),…,\left( x_{N}, Z_{N} \right) \right\}$ 为完全数据。</li>
</ul>
<p>已知 $N$ 个样本，我们希望可以求出GMM中的 $K$ 个不同的高斯分布，其中称 </p>
<ul>
<li>$\theta=\left( p_{1},p_{2},…,p_{K},\mu_{1},\mu_{2},…,\mu_{K},\Sigma_{1},\Sigma_{2},…\Sigma_{K} \right)$ 为模型参数。</li>
</ul>
<p>于是这个问题就变成了参数求解的问题，由极大似然估计的方法可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\hat{\theta}_{MLE}&=\mathop{\arg\min}_{\theta} \log P\left( X \right)\\ 
&=\mathop{\arg\min}_{\theta} \log\prod_{i=1}^{N}P\left( x_{i} \right)\\ 
&=\mathop{\arg\min}_{\theta} \sum_{i=1}^{N}{\log P\left( x_{i} \right)}\\ 
&=\mathop{\arg\min}_{\theta} \sum_{i=1}^{N}{\log \left[ \sum_{k=1}^{K}{p_{k}\cdot \mathcal N\left( x_{i} | \mu_{k},\Sigma_{k} \right)} \right]} 
\end{aligned}</script><p>其中，对数中包含加法，难以直接优化，需要采用EM算法。</p>
<h1 id="EM算法求解"><a href="#EM算法求解" class="headerlink" title="EM算法求解"></a>EM算法求解</h1><blockquote>
<p>EM算法链接</p>
</blockquote>
<p>首先明确联合概率、边缘概率和条件概率</p>
<script type="math/tex; mode=display">
P\left( x,Z \right)=P\left( Z \right)P\left( x|Z \right)=P_{Z}\cdot\mathcal N\left( x | \mu_{k},\Sigma_{k} \right)</script><script type="math/tex; mode=display">
P\left( x \right)=\sum_{k=1}^{K}{p_{k}\cdot \mathcal N\left( x | \mu_{k},\Sigma_{k} \right)}</script><script type="math/tex; mode=display">
P\left( Z|x \right)=\frac{P\left( x,Z \right)}{P\left( x \right)}</script><p>首先根据E-step，写出Q函数</p>
<script type="math/tex; mode=display">
\begin{aligned} 
Q\left( \theta,\theta^{\left( t \right)} \right)&=\int_{Z}P\left( Z|X,\theta^{\left( t \right)} \right)\log P\left( X,Z|\theta \right) \mathrm{d}Z\\ 
&=\sum_{Z}{\left[ \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right)\log\prod_{j=1}^{N}P\left( x_{j},Z_{j} | \theta \right) \right]}\\ 
&=\sum_{Z}{\left[ \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right)\sum_{j=1}^{N}{\log P\left( x_{j},Z_{j} | \theta \right)} \right]}\\ 
&=\sum_{Z}{\left[ \sum_{j=1}^{N}{\left( \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right)\log P\left( x_{j},Z_{j} | \theta \right) \right)}  \right]}\\ 
&=\sum_{j=1}^{N}{\sum_{Z}{\left[ \left( \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right) \right)\log P\left( x_{j},Z_{j} | \theta \right) \right]}} 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算过程，首先将概率写成所有样本连乘的形式，然后将对数上的连乘写成外部的连加，接下来将连加符一步步外提，直至提到最外面。</p>
</blockquote>
<p>接下来，以 $j=1$ 为例，计算内部</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad \sum_{Z}{\left[ \left( \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right) \right)\log P\left( x_{1},Z_{1} | \theta \right) \right]}\\ 
&=\sum_{Z_{1},...Z_{N}}{\left[ \left( \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right) \right)\log P\left( x_{1},Z_{1} | \theta \right) \right]}\\ 
&=\sum_{Z_{1},...Z_{N}}{\left[P\left( Z_{1}| x_{1},\theta^{\left( t \right)} \right) \prod_{i=2}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right) \log P\left( x_{1},Z_{1} | \theta \right) \right]}\\ 
&=\sum_{Z_{1}}{\left[ \left( \sum_{Z_{2},...Z_{N}}{\prod_{i=2}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right)} \right)P\left( Z_{1}| x_{1},\theta^{\left( t \right)} \right)\log P\left( x_{1},Z_{1} | \theta \right) \right]}\\ 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算过程，首先当前 $j=1$ ，从对 $Z$ 的连加中将 $Z_{1}$ 单独分类出来，同时连加内部的表达式中与 $Z_{1}$ 有关的也要分离出来。</p>
</blockquote>
<p>其中</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\quad\sum_{Z_{2},...Z_{N}}{\prod_{i=2}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right)} &=\prod_{i=2}^{N}{\sum_{Z_{i}}{P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right)}}\\ 
&=\prod_{i=2}^{N}{1}\\ 
&=1 
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad\sum_{Z_{1}}{\left[ \left( \sum_{Z_{2},...Z_{N}}{\prod_{i=2}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right)} \right)P\left( Z_{1}| x_{1},\theta^{\left( t \right)} \right)\log P\left( x_{1},Z_{1} | \theta \right) \right]}\\ 
&=\sum_{Z_{1}}{\left[ P\left( Z_{1}| x_{1},\theta^{\left( t \right)} \right)\log P\left( x_{1},Z_{1} | \theta \right) \right]} 
\end{aligned}</script><p>对 $j=1$ 如此，当 $j$ 取其它值时也有类似的上述计算。所以，对任意确定的 $j$ 都有</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad \sum_{Z}{\left[ \left( \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right) \right)\log P\left( x_{j},Z_{j} | \theta \right) \right]}\\ 
&=\sum_{Z_{j}}{\left[ P\left( Z_{j}| x_{j},\theta^{\left( t \right)} \right)\log P\left( x_{j},Z_{j} | \theta \right) \right]} 
\end{aligned}</script><p>将上式代入到Q函数</p>
<script type="math/tex; mode=display">
\begin{aligned} 
Q\left( \theta,\theta^{\left( t \right)} \right)&=\sum_{j=1}^{N}{\sum_{Z}{\left[ \left( \prod_{i=1}^{N}P\left( Z_{i}| x_{i},\theta^{\left( t \right)} \right) \right)\log P\left( x_{j},Z_{j} | \theta \right) \right]}}\\ 
&=\sum_{j=1}^{N}{\sum_{Z_{j}}{\left[ P\left( Z_{j}| x_{j},\theta^{\left( t \right)} \right)\log P\left( x_{j},Z_{j} | \theta \right) \right]}}\\ 
&=\sum_{j=1}^{N}{\sum_{k=1}^{K}{\left[ P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)\log P\left( x_{j},Z_{j}=k | \theta \right) \right]}}\\ 
&=\sum_{j=1}^{N}{\sum_{k=1}^{K}{\left[ P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)\log\left( P\left( Z_{j}=k  \right)P\left( x_{j}|Z_{j}=k , \theta \right) \right) \right]}}\\ 
&=\sum_{j=1}^{N}{\sum_{k=1}^{K}{\left[ P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)\log\left( p_{k}\cdot \mathcal N\left( x_{j} | \mu_{k},\Sigma_{k} \right) \right) \right]}}\\ 
&=\sum_{k=1}^{K}{\sum_{j=1}^{N}{\left[ P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)\log\left( p_{k}\cdot \mathcal N\left( x_{j} | \mu_{k},\Sigma_{k} \right) \right) \right]}}\\ 
\end{aligned}</script><p>解释一下上面的运算过程，就是利用联合概率和条件概率的关系，将复杂表达替换为简便表达的过程</p>
<p>然后根据M-step，对Q函数最大化，优化参数，以求 $p^{\left( t+1 \right)}=\left( p_{1}^{\left( t+1 \right)}, p_{2}^{\left( t+1 \right)},…,p_{K}^{\left( t+1 \right)}\right)$ 为例</p>
<script type="math/tex; mode=display">
\max \limits_{p}\ \sum_{k=1}^{K}{\sum_{j=1}^{N}{\left[ P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)\log\left( p_{k}\cdot \mathcal N\left( x_{j} | \mu_{k},\Sigma_{k} \right) \right) \right]}} \qquad \mathrm{s.t.}\quad\sum_{k=1}^{K}{p_{k}}=1</script><p>略去与 p 无关的项，可以写成</p>
<script type="math/tex; mode=display">
 \max \limits_{p}\ \sum_{k=1}^{K}{\sum_{j=1}^{N}{ P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)\log p_{k} }} \qquad  \mathrm{s.t.}\quad\sum_{k=1}^{K}{p_{k}}=1</script><p>写出拉格朗日函数</p>
<script type="math/tex; mode=display">
L\left( p,\lambda \right)=\sum_{k=1}^{K}{\sum_{j=1}^{N}{ P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)\log p_{k} }}+\lambda \left( \sum_{k=1}^{K}{p_{k}}-1 \right)</script><p>对求偏导</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad \frac{\partial L}{\partial p_{k}}=0\\ 
&\Rightarrow\sum_{j=1}^{N}{\frac{1}{p_{k}}P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}+\lambda = 0\\ 
&\Rightarrow\sum_{j=1}^{N}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}+\lambda p_{k} = 0\\ 
&\Rightarrow \sum_{k=1}^{K}{\sum_{j=1}^{N}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}}+\sum_{k=1}^{K}{\lambda p_{k}} = 0\\ 
&\Rightarrow \sum_{j=1}^{N}{\sum_{k=1}^{K}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}}+\lambda = 0\\ 
\end{aligned}</script><p>其中</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{K}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}=1</script><p>代入后</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad\sum_{j=1}^{N}{\sum_{k=1}^{K}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}}+\lambda = 0\\ 
&\Rightarrow \sum_{j=1}^{N}{1}+\lambda = 0\\ 
\end{aligned}</script><p>于是得到 $\lambda = -N$ ，反代入到上面的式子</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad\sum_{j=1}^{N}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}+\lambda p_{k} = 0\\ 
&\Rightarrow\sum_{j=1}^{N}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}-N p_{k} = 0\\
\end{aligned}</script><p>于是得到</p>
<script type="math/tex; mode=display">
p_{k}^{\left( t+1 \right)}=\frac{1}{N}\sum_{j=1}^{N}{P\left( Z_{j}=k| x_{j},\theta^{\left( t \right)} \right)}</script><p>至于 $\mu^{\left( t+1 \right)}$ 和 $\Sigma^{\left( t+1 \right)}$ 的求解，和上面的方法相同，甚至更简单，因为这两个参数的优化是无约束的，可以直接求偏导求解</p>
<script type="math/tex; mode=display">
\mu_{k}^{\left( t+1 \right)}=\frac{\sum_{j=1}^{N}{x_{j} P\left( Z_{j}=k|x_{j},\theta^{\left( t \right)}\right)}}{\sum_{j=1}^{N}{ P\left( Z_{j}=k|x_{j},\theta^{\left( t \right)}\right)}}</script><script type="math/tex; mode=display">
\Sigma_{k}^{\left( t+1 \right)}=\frac{\sum_{j=1}^{N}{\left( x_{j}-\mu_{k}^{\left( t \right)} \right)^{2} P\left( Z_{j}=k|x_{j},\theta^{\left( t \right)}\right)}}{\sum_{j=1}^{N}{ P\left( Z_{j}=k|x_{j},\theta^{\left( t \right)}\right)}}</script><p>其中</p>
<script type="math/tex; mode=display">
P\left( Z_{j}=k|x_{j},\theta^{\left( t \right)}\right)=\frac{p_{k}^{\left( t \right)}\cdot\mathcal{N}\left( x_{j} | \mu_{k}^{\left( t \right)},\Sigma_{k}^{\left( t \right)} \right)}{\sum_{i=1}^{K}{p_i^{\left( t \right)}\cdot\mathcal N\left( x_{j} | \mu_{i}^{\left( t \right)},\Sigma_{i}^{\left( t \right)} \right)}}</script>]]></content>
      <categories>
        <category>机器学习算法与模型</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>GMM</tag>
      </tags>
  </entry>
  <entry>
    <title>Adaboost算法及代码实现</title>
    <url>/2020/07/07/Adaboost%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h1><p>输入：训练集 $T=\left\{ \left( x_{1},y_{1} \right),\left( x_{2},y_{2} \right),…,\left( x_{N},y_{N} \right) \right\}$ ，其中 $x_{i}\in\mathbb{R}^{n},y_{i}\in\left\{ -1,+1 \right\}$ </p>
<p>输出：最终加权组合起来的分类器</p>
<a id="more"></a>
<p>算法：</p>
<ol>
<li>初始化权重 <script type="math/tex; mode=display">
D_{1}=\left\{ w_{1i} \right\}_{i=1}^{N}</script>其中 <script type="math/tex; mode=display">
w_{1i}=\frac{1}{N}</script></li>
<li>训练 $M$ 个基分类器<ol>
<li>用带权重 $D_{m}$ 的训练集组训练出一个基分类器 <script type="math/tex; mode=display">
G_{m}\left( x \right)</script></li>
<li>计算这个基分类器的错误率（带权重） <script type="math/tex; mode=display">
e_{m}=\sum_{i=1}^{N}{w_{mi}I\left( G_{m}\left( x_{i} \right)\ne y_{i} \right)}</script></li>
<li>计算分类器权重： <script type="math/tex; mode=display">
\alpha_{m} =\frac{1}{2}\log\frac{1-e_{m}}{e_{m}}</script></li>
<li>计算规范化因子 <script type="math/tex; mode=display">
Z_{m}=\sum_{i=1}^{N}{w_{mi}\exp\left( -\alpha_{m}y_{i}G_{m}\left( x_{i} \right) \right)}</script></li>
<li>更新权重： <script type="math/tex; mode=display">
D_{m+1}=\left\{ w_{m+1,i} \right\}_{i=1}^{N}</script>其中 <script type="math/tex; mode=display">
w_{m+1,i}=\frac{w_{mi}}{Z_{m}}\exp\left( -\alpha_{m}y_{i}G_{m}\left( x_{i} \right) \right)</script></li>
</ol>
</li>
<li>组合得到最终分类器<script type="math/tex; mode=display">
G\left( x \right)={\rm sign}\left( \sum_{m=1}^{M}{\alpha_{m}G_{m}\left( x \right)} \right)</script></li>
</ol>
<h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><p>参考代码：<a href="https://github.com/fengdu78/lihang-code/blob/master/第08章%20提升方法/8.Boost.ipynb">https://github.com/fengdu78/lihang-code/blob/master/第08章%20提升方法/8.Boost.ipynb</a></p>
<p>AdaBoost构建框架如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdaBoost</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num, datasets, labels</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化模型参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">G_train</span>(<span class="params">self, features, labels</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练单个基分类器</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">G</span>(<span class="params">self, x, v, flag</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        应用单个的基分类器</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_alpha</span>(<span class="params">self, err</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算基分类器权重</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Z</span>(<span class="params">self, a, res</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算规范化因子</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">W</span>(<span class="params">self, a, res, Z</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新权重</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        adaboost算法训练过程</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, feature</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        单样本预测</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>接下来详细解释几个重要函数。</p>
<p>首先是初始化函数，涉及到贯穿整个算法的一些参数，包括</p>
<ul>
<li>基分类器数量</li>
<li>数据集（包括特征值、类别、样本数量、特征数量）</li>
<li>数据权重集合</li>
<li>基分类器参数集合</li>
<li>基分类器权重集合</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num, datasets, labels</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化模型参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.clf_num = num</span><br><span class="line">    self.X, self.Y = datasets, labels</span><br><span class="line">    <span class="comment"># M个数据，也就是N个特征</span></span><br><span class="line">    self.M, self.N = datasets.shape</span><br><span class="line">    <span class="comment"># 分类器参数集合</span></span><br><span class="line">    self.clf_args = []</span><br><span class="line">    <span class="comment"># 初始化数据权重和分类器权重</span></span><br><span class="line">    self.w = [<span class="number">1.0</span> / self.M] * self.M</span><br><span class="line">    self.alpha = []</span><br></pre></td></tr></table></figure>
<p>接下来是计算分类器权重、规范化因子和数据权重更新的函数，可以按照前文中的公式计算</p>
<ul>
<li>计算分类器权重时需要的参数：当前分类器的错误率</li>
<li>计算规范化因子时需要的参数：当前分类器权重、当前分类器的分类结果</li>
<li>计算新一轮权重时需要的参数：当前分类器权重、当前分类器的分类结果、规范化因子</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_alpha</span>(<span class="params">self, err</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算分类器权重，为了防止err为0，也可以在分母加一个极小值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.log((<span class="number">1</span> - err) / err)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Z</span>(<span class="params">self, a, res</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算规范化因子</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    numerators = [self.w[i] * np.exp(-<span class="number">1</span> * a * self.Y[i] * res[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.M)]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(numerators)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">W</span>(<span class="params">self, a, res, Z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    更新权重</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.M):</span><br><span class="line">        self.weights[i] = self.w[i] * np.exp(-<span class="number">1</span> * a * self.Y[i] * res[i]) / Z </span><br></pre></td></tr></table></figure>
<p>接下来是样本预测函数，也就是组合得到的最终的分类器，加权求和后使用符号函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, feature</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    单样本预测</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    res = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.clf_num):</span><br><span class="line">        j, v, flag = self.clf_args[i]</span><br><span class="line">        result += self.alpha * self.G(feature[j], v, flag)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> res &gt; <span class="number">0</span> <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>最重要就是AdaBoost算法训练过程，其中基分类器用的是单层决策树，循环嵌套结构如下</p>
<ul>
<li>迭代训练每个基分类器</li>
<li>训练一个基分类器时，由于是单层决策树，要尝试使用每一个特征来分类，选一个最好的</li>
<li>用一个特征分类时，要尝试每个分类边界值，找到一个最好的（这一步体现在基分类器训练中）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    adaboost算法训练过程</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 迭代训练每个基分类器</span></span><br><span class="line">    <span class="comment"># 每个基分类器只用N个特征中对一个来分类，从中挑一个最好的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.clf_num):</span><br><span class="line">        <span class="comment"># 如果使用sklearn的决策树，就不需要尝试每个特征，也不需要记录基分类器参数，例如</span></span><br><span class="line">        <span class="comment"># clf = DecisionTreeClassifier(max_depth=1)</span></span><br><span class="line">        <span class="comment"># clf.fit(self.X, self.Y)</span></span><br><span class="line">        <span class="comment"># best_res = clf.predict(self.X)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化最低错误率、最佳分类边界值、最佳分类结果、分为正类的判断方向（大于或者小于）</span></span><br><span class="line">        min_err, boundary, best_res, flag, feature_num = <span class="number">1</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 尝试每个特征进行分类器训练</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.N):</span><br><span class="line">            features = self.X[:, j]</span><br><span class="line">            <span class="comment"># 用带权重的数据训练一个基分类器</span></span><br><span class="line">            v, curr_flag, err, res = self.G_train(features, self.Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果错误率比已有记录更低，就更新记录</span></span><br><span class="line">            <span class="keyword">if</span> err &lt; min_err:</span><br><span class="line">                min_err = err</span><br><span class="line">                feature_num = j</span><br><span class="line">                boundary = v </span><br><span class="line">                flag = curry_flag</span><br><span class="line">                best_res = res</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 如果错误率为0，就不需要再尝试其他特征了</span></span><br><span class="line">            <span class="keyword">if</span> min_err == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果使用sklearn的决策树模型，就记录下模型即可，例如</span></span><br><span class="line">        <span class="comment"># self.clfs.append(clf)</span></span><br><span class="line">        <span class="comment"># 记录下当前分类器参数（参数包括分类特征、分类边界值，分类结果）</span></span><br><span class="line">        self.clf_args.append((feature_num, boundary, final_flag))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据当前分类器的错误率计算分类器权重并记录</span></span><br><span class="line">        a = self.cal_alpha(min_err)</span><br><span class="line">        self.alpha.append(a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据分类器权重和分类结果，计算归一化因子并更新权重</span></span><br><span class="line">        Z = self.Z(a, best_res)</span><br><span class="line">        self.W(a, best_res, Z)</span><br></pre></td></tr></table></figure>
<p>单层决策树分类器不是AdaBoost算法的重点，代码也不再详细说明了，可以使用sklearn库中的决策树模型，将参数max_depth设置为1，这样也是一个基分类器。</p>
]]></content>
      <categories>
        <category>机器学习算法与模型</category>
      </categories>
      <tags>
        <tag>Boost</tag>
        <tag>树模型</tag>
      </tags>
  </entry>
  <entry>
    <title>隐马尔可夫模型（HMM）数学推导思路</title>
    <url>/2020/07/21/HMM%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/</url>
    <content><![CDATA[<h1 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h1><p>本文主要从详细的数学推导角度，讲解HMM模型，主要内容包括HMM模型的</p>
<ul>
<li>基本构成元素（状态序列与状态值、观测序列与观测值）</li>
<li>三个参数（初始概率分布、转移矩阵、发射矩阵）</li>
<li>两个假设（齐次马尔可夫假设、观测独立假设）</li>
<li>三个问题（概率计算、参数学习、状态预测）</li>
<li>四个算法（Forward、Backward、Baum-Welch、Viterbi）</li>
</ul>
<a id="more"></a>
<h1 id="模型简介"><a href="#模型简介" class="headerlink" title="模型简介"></a>模型简介</h1><!--[HMM模型示意图](HMM.png)-->
<h2 id="状态序列和状态值"><a href="#状态序列和状态值" class="headerlink" title="状态序列和状态值"></a>状态序列和状态值</h2><p>上图中 $I$ 表示一个状态序列，长度为 $T$ ， $i_{t}$ 表示状态序列中的第 $t$ 个状态值。状态值的取值集合为 $Q=\left\{ q_{1},q_{2},…,q_{N} \right\}$ ，也就是每个状态有 $N$ 种不同的取值，任意 $i_{t}\in Q$ 。词性标注问题中，词性就是状态值，一句话分词后的序列被标注后，得到的词性序列就是状态序列。</p>
<h2 id="观测序列和观测值"><a href="#观测序列和观测值" class="headerlink" title="观测序列和观测值"></a>观测序列和观测值</h2><p>上图中 $O$ 表示一个观测序列，长度也为 $T$ ， $o_{t}$ 表示序列中的第 $t$ 个观测值。观测值的取值集合为 $V=\left\{ v_{1},v_{2},…,v_{M} \right\}$ ，也就是观测变量有 $M$ 种不同的取值，任意 $o_{t}\in V$ 。词性标注问题中，连续的分词结果就是观测序列，其中每个词就是一个观测值。</p>
<h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><p>HMM模型参数为 $\lambda = \left( \pi,A,B \right)$ ，其中包括：</p>
<ol>
<li>初始概率分布 $\pi=\left( \pi_{1},\pi_{2},…,\pi_{N} \right)$ ，其中 $\pi_{k}=P\left( i_{1}=q_{k} \right)$ 。初状态取值为 $q_{k}$ 的概率，显然满足 $\sum_{k=1}^{N}{\pi_{k}}=1$ </li>
<li>转移矩阵 $A=\left[ a_{jk} \right]$ ，其中 $j=1,…,N$ ， $k=1,…,N$ ， $a_{jk}=P\left( i_{t+1}=q_{k}|i_{t}=q_{j} \right)$ 。某一个状态取值为 $q_{j}$ 的条件下，下一个状态取值为 $q_{k}$ 的概率，也就是上图中第一排节点之间的横向箭头的含义，显然满足 $\sum_{k=1}^{N}{a_{jk}}=1$ </li>
<li>发射矩阵 $B=\left[ b_{j}\left( k \right) \right]$ ，其中 $j=1,…,M$ ， $k=1,…,N$ ，  $b_{j}\left( k \right)=P\left( o_{t}=v_{k}|i_{t}=q_{j} \right)$ 。某一个状态取值为 $q_{j}$ 的条件下，对应的观测值为 $v_{k}$ 的概率，也就是上图中第一排节点和第二排节点之间的纵向箭头的含义，显然满足 $\sum_{k=1}^{N}{b_{j}\left( k \right)}=1$ </li>
</ol>
<p><strong>自然语言处理中，分词问题（给每个字标注“词首”、“词中”、“词尾”）、词性标注问题（给每个词标注“名词”、“动词”、“形容词”等）都可以套用HMM模型，后续的算法推导，可以将应用场景代入，便于理解。</strong></p>
<h1 id="模型假设"><a href="#模型假设" class="headerlink" title="模型假设"></a>模型假设</h1><p>两个重要假设是简化后续数学推导的关键，会多次重复使用</p>
<h2 id="齐次马尔可夫假设"><a href="#齐次马尔可夫假设" class="headerlink" title="齐次马尔可夫假设"></a>齐次马尔可夫假设</h2><script type="math/tex; mode=display">
P\left( i_{t+1}|i_{1},i_{2},...,i_{t},o_{1},o_{2},...,o_{t} \right)=P\left( i_{t+1}|i_{t} \right)</script><p>其含义为：下一状态的取值只与当前状态值有关，与更早的状态值或观察值无关。从HMM的模型示意图中可以看出来，第一排的每一个状态节点，只有前一个状态节点指向它，没有其他任何节点再指向它。</p>
<h2 id="观测独立假设"><a href="#观测独立假设" class="headerlink" title="观测独立假设"></a>观测独立假设</h2><script type="math/tex; mode=display">
P\left( o_{t}|i_{1},i_{2},...,i_{t},o_{1},o_{2},...,o_{t} \right)=P\left( o_{t}|i_{t} \right)</script><p>其含义为：当前的观测值只与当前状态值有关，与更早的状态值或者观测值无关。从HMM的模型示意图中可以看出来，第二排的每一个观测节点，只有上方与之对应的状态节点指向它，没有其他任何节点再指向它。</p>
<p>另外，为了简化运算，后续还会多次用到联合概率和边缘概率的关系、联合概率和条件概率的关系。</p>
<h1 id="问题一-概率计算"><a href="#问题一-概率计算" class="headerlink" title="问题一  概率计算"></a>问题一  概率计算</h1><p>这个问题就是已知一个确定的HMM模型，求一个观测序列 $O$ 出现的概率，也就是 $P\left( O|\lambda \right)$ </p>
<h2 id="直接计算"><a href="#直接计算" class="headerlink" title="直接计算"></a>直接计算</h2><script type="math/tex; mode=display">
\begin{aligned} 
P\left( O|I \right) &= \sum_{I}{P\left( O, I|\lambda \right)}\\ 
&=\sum_{I}{P\left( O |I,\lambda \right)P\left( I|\lambda \right)}\\ 
\end{aligned}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left( I|\lambda \right)&=P\left(i_{1},i_{2},...,i_{T} |\lambda \right)\\ 
&=P\left(i_{T}|i_{1},i_{2},...,i_{T-1}, \lambda \right)P\left(i_{1},i_{2},...,i_{T-1}| \lambda \right)\\ 
&=P\left(i_{T}|i_{T-1}, \lambda \right)P\left(i_{1},i_{2},...,i_{T-1}| \lambda \right)\\ 
&=a_{i_{T-1}i_{T}}P\left(i_{1},i_{2},...,i_{T-1}| \lambda \right) 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算。利用条件概率和联合概率的关系、齐次马尔可夫假设进行运算。</p>
</blockquote>
<p>同理</p>
<script type="math/tex; mode=display">
P\left(i_{1},i_{2},...,i_{T-1} |\lambda \right)=a_{i_{T-2}i_{T-1}}P\left(i_{1},i_{2},...,i_{T-2}| \lambda \right)</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left( I|\lambda \right)&=a_{i_{T-1}i_{T}}P\left(i_{1},i_{2},...,i_{T-1}| \lambda \right)\\ 
&=a_{i_{T-1}i_{T}}\cdot a_{i_{T-2}i_{T-1}}\cdot a_{i_{T-3}i_{T-2}}...a_{i_{1}i_{2}}\cdot P\left( i_{1}|\lambda \right)\\ 
&=a_{i_{T-1}i_{T}}\cdot a_{i_{T-2}i_{T-1}}\cdot a_{i_{T-3}i_{T-2}}...a_{i_{1}i_{2}}\cdot  \pi_{i_{1}}\\ 
&=\pi_{i_{1}}\prod_{t=2}^{T}{a_{i_{t-1}i_{t}}} 
\end{aligned}</script><p>另外</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left( O|I,\lambda \right)&=P\left(o_{1},o_{2},...,o_{T} |i_{1},i_{2},...,i_{T},\lambda \right)\\ 
&=P\left(o_{1}|o_{2},...,o_{T},i_{1},i_{2},...,i_{T},\lambda \right)P\left(o_{2},...,o_{T}|i_{1},i_{2},...,i_{T},\lambda \right)\\ 
&=P\left(o_{1}|i_{1},\lambda \right)P\left(o_{2},...,o_{T}|i_{1},i_{2},...,i_{T},\lambda \right)\\ 
&=b_{i_{1}}\left( o_{1} \right)P\left(o_{2},...,o_{T}|i_{1},i_{2},...,i_{T},\lambda \right) 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算。利用条件概率和联合概率的关系、观测独立假设进行运算。</p>
</blockquote>
<p>其中</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left(o_{2},...,o_{T}|i_{1},i_{2},...,i_{T},\lambda \right)&=P\left( o_{2}|i_{2} \right)P\left(o_{3},...,o_{T}|i_{1},i_{2},...,i_{T},\lambda \right)\\ 
&=b_{i_{2}}\left( o_{2}\right)P\left(o_{3},...,o_{T}|i_{1},i_{2},...,i_{T},\lambda \right) 
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left( O|I,\lambda \right)&=b_{i_{1}}\left( o_{1} \right)P\left(o_{2},...,o_{T}|i_{1},i_{2},...,i_{T},\lambda \right)\\ 
&=b_{i_{1}}\left( o_{1} \right)\cdot b_{i_{2}}\left( o_{2} \right)...b_{i_{T-1}}\left( o_{T-1} \right)\cdot b_{i_{T}}\left( o_{T} \right)\\ 
&=\prod_{t=1}^{T}{b_{i_{t}}\left( o_{t} \right)} 
\end{aligned}</script><p>综上</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left( O|I \right) &=\sum_{I}{P\left( O |I,\lambda \right)P\left( I|\lambda \right)}\\ 
&=\sum_{I}{\left[ \pi_{i_{1}}\left( \prod_{t=2}^{T}{a_{i_{t-1}i_{t}}} \right)\left( \prod_{t=1}^{T}{b_{i_{t}}\left( o_{t} \right)} \right) \right]} 
\end{aligned}</script><p>这样的计算复杂度太高，不适合计算机，我们对以上计算进行改进，设计两个基于递推关系的动态规划方法，分别是前向算法和后向算法</p>
<h2 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h2><p>我们定义</p>
<script type="math/tex; mode=display">
\alpha_{t}\left( k \right) = P\left( o_{1},o_{2},...,o_{t},i_{t}=q_{k} | \lambda \right)</script><p>所以就有</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left( O|\lambda \right) &= \sum_{k=1}^{N}{P\left( o_{1},o_{2},...,o_{T},i_{T}=q_{k} | \lambda \right)}\\ 
&=\sum_{k=1}^{N}{\alpha_{T}\left( k \right)} 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算。利用边缘概率和联合概率的关系进行运算。</p>
</blockquote>
<p>推导递推关系</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\alpha_{t+1}\left( k \right) &=P\left( o_{1},...,o_{t},o_{t+1},i_{t+1}=q_{k} | \lambda \right)\\ 
&=\sum_{j=1}^{N}{P\left( o_{1},...,o_{t},o_{t+1},i_{t+1}=q_{k},i_{t}=q_{j} | \lambda \right)}\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1}|o_{1},...,o_{t},i_{t+1}=q_{k},i_{t}=q_{j}, \lambda \right)P\left(o_{1},...,o_{t},i_{t+1}=q_{k},i_{t}=q_{j} | \lambda \right)}\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1}| i_{t+1}=q_{k}, \lambda \right)P\left(o_{1},...,o_{t},i_{t+1}=q_{k},i_{t}=q_{j} | \lambda \right)}\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1}| i_{t+1}=q_{k}, \lambda \right)P\left(i_{t+1}=q_{k} |i_{t}=q_{j}, o_{1},...,o_{t},\lambda \right)P\left(i_{t}=q_{j}, o_{1},...,o_{t}|\lambda \right)}\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1}| i_{t+1}=q_{k}, \lambda \right)P\left(i_{t+1}=q_{k} |i_{t}=q_{j},\lambda \right)P\left(i_{t}=q_{j}, o_{1},...,o_{t}|\lambda \right)}\\ 
&=\sum_{j=1}^{N}{b_{k}\left( o_{t+1} \right)\cdot a_{jk}\cdot \alpha_{t}\left( j \right)}\\ 
&=b_{k}\left( o_{t+1} \right)\cdot\sum_{j=1}^{N}{a_{jk}\cdot \alpha_{t}\left( j \right)}\\ 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算。先利用边缘概率和联合概率的关系进行构造，利用条件概率和联合概率的关系进行提取，再利用两个假设进行运算化简。</p>
</blockquote>
<p>得到递推关系后，就可以写出前向算法流程</p>
<p>输入：模型参数 $\lambda$ （初始概率分布 $\pi$ 、转移矩阵 $A$  、发射矩阵 $B$ ），一个观测序列 $O$ </p>
<p>输出： $P\left( O|\lambda \right)$</p>
<p>算法过程：</p>
<ol>
<li>构造一个 $T\times N$ 的动态规划矩阵，用于存储所有 $\alpha_{t}\left( k \right), t=1,…,T, k=1,…,N$ ，并初始化 <script type="math/tex; mode=display">
\alpha_{1}\left( k \right)=\pi_{k}\cdot b_{k}\left( o_{1} \right)</script></li>
<li>递推求解，依次对所有 $t=1,…,T-1$ ，遍历每一个 $k=1,…,N$ ，递推计算 <script type="math/tex; mode=display">\alpha_{t+1}\left( k \right)=b_{k}\left( o_{t+1} \right)\cdot\sum_{j=1}^{N}{a_{jk}\cdot \alpha_{t}\left( j \right)}</script></li>
<li>最后计算 <script type="math/tex; mode=display">
P\left( O|\lambda \right)=\sum_{k=1}^{N}{\alpha_{T}\left( k \right)}</script></li>
</ol>
<p>以上就是前向算法，可以看出，空间复杂度为 $O\left( T\times N \right)$ ，时间复杂度为 $O\left( T\times N\times N \right)$ </p>
<h2 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h2><p>和前向算法类似，定义</p>
<script type="math/tex; mode=display">
\beta_{t}\left( k \right)=P\left( o_{t+1},o_{t+2},...,o_{T}|i_{t}=q_{k},\lambda \right)</script><p>所以有</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P\left( O|\lambda \right)&= \sum_{k=1}^{N}{P\left( o_{1},o_{2},...,o_{T},i_{1}=q_{k} | \lambda \right)}\\ 
&= \sum_{k=1}^{N}{P\left( o_{1},o_{2},...,o_{T}|i_{1}=q_{k}, \lambda \right)P\left(i_{1}=q_{k}| \lambda \right)}\\ 
&=\sum_{k=1}^{N}{P\left( o_{1}|o_{2},...,o_{T},i_{1}=q_{k}, \lambda \right)P\left(o_{2},...,o_{T}|i_{1}=q_{k}, \lambda \right)P\left(i_{1}=q_{k}| \lambda \right)}\\ 
&=\sum_{k=1}^{N}{P\left( o_{1}|i_{1}=q_{k}, \lambda \right)P\left(o_{2},...,o_{T}|i_{1}=q_{k}, \lambda \right)P\left(i_{1}=q_{k}| \lambda \right)}\\ 
&=\sum_{k=1}^{N}{b_{k}\left( o_{1} \right)\beta_{1}\left( k \right) \pi_{k}}\\ 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算。先利用边缘概率和联合概率的关系进行构造，利用条件概率和联合概率的关系进行提取，再利用观测独立假设进行运算化简。</p>
</blockquote>
<p>推导递推关系</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\beta_{t}\left( k \right)&=P\left( o_{t+1},o_{t+2},...,o_{T}|i_{t}=q_{k},\lambda \right)\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1},o_{t+2},...,o_{T},i_{t+1}=q_{j}|i_{t}=q_{k},\lambda \right)}\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1},o_{t+2},...,o_{T}|i_{t+1}=q_{j},i_{t}=q_{k},\lambda \right)P\left( i_{t+1}=q_{j}|i_{t}=q_{k},\lambda \right)}\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1}|o_{t+2},...,o_{T},i_{t+1}=q_{j},\lambda \right)P\left( o_{t+2},...,o_{T}|i_{t+1}=q_{j},\lambda \right)P\left( i_{t+1}=q_{j}|i_{t}=q_{k},\lambda \right)}\\ 
&=\sum_{j=1}^{N}{P\left( o_{t+1}|i_{t+1}=q_{j},\lambda \right)P\left( o_{t+2},...,o_{T}|i_{t+1}=q_{j},\lambda \right)P\left( i_{t+1}=q_{j}|i_{t}=q_{k},\lambda \right)}\\ 
&=\sum_{j=1}^{N}{b_{j}\left( o_{t+1} \right)\cdot\beta_{t+1}\left( j \right)\cdot a_{kj}}\\ 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算。先利用边缘概率和联合概率的关系进行构造，利用条件概率和联合概率的关系进行提取，再利用观测独立假设进行运算化简。</p>
</blockquote>
<p>得到递推关系后，就可以写出后向算法流程</p>
<p>输入：模型参数 $\lambda$ （初始概率分布 $\pi$ 、转移矩阵 $A$  、发射矩阵 $B$ ），一个观测序列 $O$ </p>
<p>输出： $P\left( O|\lambda \right)$ </p>
<p>算法过程：</p>
<ol>
<li>构造一个 $T\times N$ 的动态规划矩阵，用于存储所有 $\beta_{t}\left( k \right), t=1,…,T, k=1,…,N$ ，并初始化 <script type="math/tex; mode=display">
\alpha_{T}\left( k \right)=1</script></li>
<li>递推求解，依次对所有 $t=T-1,…,1$ ，遍历每一个 $k=1,…,N$ ，递推计算 <script type="math/tex; mode=display">
\beta_{t}\left( k \right)=\sum_{j=1}^{N}{a_{kj}\cdot b_{j}\left( o_{t+1} \right)\cdot\beta_{t+1}\left( j \right)}</script></li>
<li>最后计算 <script type="math/tex; mode=display">
P\left( O|\lambda \right)=\sum_{k=1}^{N}{\pi_{k}\cdot b_{k}\left( o_{1} \right)\cdot\beta_{1}\left( k \right)}</script></li>
</ol>
<p>以上就是后向算法，可以看出，空间复杂度为 $O\left( T\times N \right)$ ，时间复杂度为 $O\left( T\times N\times N \right)$</p>
<h2 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h2><p>为了帮助理解，给出下图</p>
<!-- [前向算法、后向算法图示](ForwardBackward.png)-->
<p>如图，选择任意一个时刻 $t=1,…,T-1$ ，假设 $i_{t}=q_{j}$ ， $i_{t+1}=q_{k}$ ，其中 $j,k=1,…,N$ 。左侧黄色直角框表示的就是 $\alpha_{t}\left( j \right)$ ，右侧黄色钝角框表示的就是 $\beta_{t+1}\left( k \right)$ ，根据这张图也可以理解如下表达式</p>
<script type="math/tex; mode=display">
P\left( O|\lambda \right)=\sum_{j=1}^{N}{\sum_{k=1}^{N}{\left[ \alpha_{t}\left( j \right)\cdot a_{jk}\cdot b_{k}\left( o_{t+1} \right)\cdot\beta_{t+1}\left( k \right) \right]}}</script><p>这张图也能帮助理解后面 $\gamma_{t}\left( k \right)$ 和 $\xi_{t}\left( j,k \right)$ 的表达含义。</p>
<h1 id="问题二-模型学习"><a href="#问题二-模型学习" class="headerlink" title="问题二  模型学习"></a>问题二  模型学习</h1><p>这个问题就是给定样本数据，求模型的参数。如果样本中只包含观测序列，那么这就是一个无监督学习过程；如果样本中包含观测序列及其对应的状态序列，那么这就是一个监督学习过程。</p>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>当我们拥有了观测序列及对应的状态序列，我们就可以根据统计信息，以频率近似概率，直接快速计算参数。所以需要非常清楚三个参数的具体意义。</p>
<p>首先，初始概率分布。假设 $M_{k}$ 表示样本中以 $q_{k}$ 为起始点出现的次数，那么</p>
<script type="math/tex; mode=display">
\hat{\pi}_{k}=\frac{M_{k}}{\sum_{k=1}^{N}{M_{k}}}</script><p>其次，转移矩阵。假设 $M_{jk}$ 表示样本中从 $q_{j}$ 状态直接转移到 $q_{k}$ 状态出现的次数，那么</p>
<script type="math/tex; mode=display">
\hat{a}_{jk}=\frac{M_{jk}}{\sum_{k=1}^{N}{M_{jk}}}</script><p>最后，发射矩阵。假设 $M_{jk}$ 表示样本中由 $q_{j}$ 状态直接得到观测值 $v_{k}$ 出现的次数，那么</p>
<script type="math/tex; mode=display">
\hat{b}_{j}\left( k \right)=\frac{M_{jk}}{\sum_{k=1}^{N}{M_{jk}}}</script><h2 id="无监督学习——-Baum-Welch算法（EM）"><a href="#无监督学习——-Baum-Welch算法（EM）" class="headerlink" title="无监督学习—— Baum-Welch算法（EM）"></a>无监督学习—— Baum-Welch算法（EM）</h2><p>样本数据中不包含状态序列时，就需要使用优化算法。Baum-Welch算法实际上就是EM算法。接下来的推导直接套用EM算法的结论，复习见下方链接。</p>
<blockquote>
<p>EM算法链接</p>
</blockquote>
<p>直接套用，注意上角标出现的 $\left( t \right)$ 表示第 $t$ 轮迭代优化后的参数， $\lambda^{\left( t \right)}=\left( \pi^{\left( t \right)},A^{\left( t \right)},B^{\left( t \right)} \right)$ ，小角标出现的 $t$ 表示序列中的第 $t$ 时刻</p>
<script type="math/tex; mode=display">
\begin{aligned}  \lambda^{\left( t+1 \right)}&=\mathop{\arg\max}_{\lambda}\ \sum_{I}{P\left( I|O,\lambda^{\left( t \right)} \right)\log{P\left( O,I|\lambda \right)}}\\ &= \mathop{\arg\max}_{\lambda}\ \sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)P\left( O|\lambda^{\left( t \right)} \right)\log{P\left( O,I|\lambda \right)}}\\ &= \mathop{\arg\max}_{\lambda}\ \sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\log{P\left( O,I|\lambda \right)}}\\ \end{aligned}</script><blockquote>
<p>解释一下上面的运算。第二行到第三行， P\left( O|\lambda^{\left( t \right)} \right) 是常数，可以舍去。</p>
</blockquote>
<p>其中</p>
<script type="math/tex; mode=display">
P\left( O,I|\lambda \right)=\pi_{i_{1}}\cdot \prod_{t=2}^{T}a_{i_{t-1}i_{t}} \cdot  \prod_{t=1}^{T}b_{i_{t}}\left( o_{t} \right)</script><p>代入Q函数进行运算</p>
<script type="math/tex; mode=display">
\begin{aligned} Q\left( \lambda, \lambda^{\left( t \right)} \right)&=\sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\log{P\left( O,I|\lambda \right)}}\\ &=\sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\log{\left( \pi_{i_{1}}\cdot \prod_{t=2}^{T}a_{i_{t-1}i_{t}} \cdot  \prod_{t=1}^{T}b_{i_{t}}\left( o_{t} \right)  \right)}}\\ &=\sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\left( \log\pi_{i_{1}}+ \log\prod_{t=2}^{T}a_{i_{t-1}i_{t}} + \log \prod_{t=1}^{T}b_{i_{t}}\left( o_{t} \right)  \right)}\\ &=\sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\left( \log\pi_{i_{1}}+ \sum_{t=2}^{T}{\log a_{i_{t-1}i_{t}}} + \sum_{t=1}^{T}{\log b_{i_{t}}\left( o_{t} \right)}  \right)}\\ &=\sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\log\pi_{i_{1}}}+\sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\sum_{t=2}^{T}{\log a_{i_{t-1}i_{t}}}}+\sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\sum_{t=1}^{T}{\log b_{i_{t}}\left( o_{t} \right)} }\\ \end{aligned}</script><blockquote>
<p>解释一下上面的运算。对数中真数连乘改写为对数连加，将三种参数分离。</p>
</blockquote>
<p>以求参数 $\pi$ 为例</p>
<script type="math/tex; mode=display">
\begin{aligned} \pi^{\left( t+1 \right)}&= \mathop{\arg\max}_{\pi}\ Q\left( \lambda, \lambda^{\left( t \right)} \right)\\ &=\mathop{\arg\max}_{\pi}\ \sum_{I}{P\left( I,O|\lambda^{\left( t \right)} \right)\log\pi_{i_{1}}}\\ &=\mathop{\arg\max}_{\pi}\ \sum_{i_{1}}{...\sum_{i_{T}}{P\left( i_{1},...,i_{T},O|\lambda^{\left( t \right)} \right)\log\pi_{i_{1}}}}\\ &=\mathop{\arg\max}_{\pi}\ \sum_{i_{1}}{\log\pi_{i_{1}}\sum_{i_{2}...i_{T}}{P\left( i_{1},i_{2},...,i_{T},O|\lambda^{\left( t \right)} \right)}}\\ &=\mathop{\arg\max}_{\pi}\ \sum_{i_{1}}{P\left( i_{1},O|\lambda^{\left( t \right)} \right)\log\pi_{i_{1}}}\\ &=\mathop{\arg\max}_{\pi}\ \sum_{k=1}^{N}{P\left( i_{1}=q_{k},O|\lambda^{\left( t \right)} \right)\log\pi_{k}}\\ \end{aligned}</script><blockquote>
<p>解释一下上面的运算。将连加部分展开，舍去与参数优化无关的项。</p>
</blockquote>
<p>并且有约束条件</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{N}{\pi_{k}}=1</script><p>写出拉格朗日函数</p>
<script type="math/tex; mode=display">
L\left( \pi, \eta \right)=\sum_{k=1}^{N}{P\left( i_{1}=q_{k},O|\lambda^{\left( t \right)} \right)\log\pi_{k}}+\eta\left( \sum_{k=1}^{N}{\pi_{k}}-1 \right)</script><p>求偏导数，令其为0</p>
<script type="math/tex; mode=display">
\begin{aligned} &\quad \frac{\partial L}{\partial \pi_{k}}=\frac{P\left( O,i_{1}=q_{k}|\lambda^{\left( t \right)} \right)}{\pi_{k}}+\eta=0\\ &\Rightarrow P\left( O,i_{1}=q_{k}|\lambda^{\left( t \right)} \right)+\eta\pi_{k}=0\\ &\Rightarrow \sum_{k=1}^{N}{P\left( O,i_{1}=q_{k}|\lambda^{\left( t \right)} \right)}+\sum_{k=1}^{N}{\eta\pi_{k}}=0\\ &\Rightarrow P\left( O|\lambda^{\left( t \right)} \right)+\eta=0\\ &\Rightarrow \eta = -P\left( O|\lambda^{\left( t \right)} \right) \end{aligned}</script><p>将 $\eta$ 反代回到第二行，得到</p>
<script type="math/tex; mode=display">
\pi_{k}=\frac{P\left( O,i_{1}=q_{k}|\lambda^{\left( t \right)} \right)}{P\left( O|\lambda^{\left( t \right)} \right)}</script><p>同样的方法求解转移矩阵和发射矩阵，得到</p>
<script type="math/tex; mode=display">
a_{jk}=\frac{\sum_{t=1}^{T-1}{P\left( O,i_{t}=q_{j},i_{t+1}=q_{k}|\lambda^{\left( t \right)}  \right)}}{\sum_{t=1}^{T-1}{P\left( O,i_{t}=q_{j},|\lambda^{\left( t \right)}  \right)}}</script><script type="math/tex; mode=display">
b_{j}\left( k \right)=\frac{\sum_{t=1}^{T-1}{P\left( O,o_{t}=v_{k},i_{t}=q_{j},|\lambda^{\left( t \right)}  \right)}}{\sum_{t=1}^{T-1}{P\left( O,i_{t}=q_{j},|\lambda^{\left( t \right)}  \right)}}</script><p>显然，这些参数的计算要依赖于前面的概率计算问题。为例和前面的前向算法、后向算法关联，方便代码编写，引入下面一些表达</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\gamma_{t}\left( k \right)
&=P\left( i_{t}=q_{k}|O, \lambda \right)\\ 
&=\frac{P\left( i_{t}=q_{k},O|\lambda \right)}{P\left( O| \lambda \right)}\\ 
&=\frac{\alpha_{t}\left( k \right)\beta_{t}\left( k \right)}{\sum_{j=1}^{N}{\alpha_{t}\left( j \right)\beta_{t}\left( j \right)}}\\ 
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}  
\xi_{t}\left( j,k \right)
&=P\left( i_{t}=q_{j},i_{t+1}=q_{k}|O,\lambda \right)\\ 
&=\frac{P\left( i_{t}=q_{j},i_{t+1}=q_{k},O|\lambda \right)}{P\left( O|\lambda \right)}\\ 
&=\frac{\alpha_{t}\left( j \right)\cdot a_{jk}\cdot b_{k}\left( o_{t+1} \right)\cdot\beta_{t+1}\left( k \right)}{\sum_{j=1}^{N}{\sum_{k=1}^{N}{\left[ \alpha_{t}\left( j \right)\cdot a_{jk}\cdot b_{k}\left( o_{t+1} \right)\cdot\beta_{t+1}\left( k \right) \right]}}}  
\end{aligned}</script><p>于是有</p>
<script type="math/tex; mode=display">
\pi_{k}=\gamma_{1}\left( k \right)</script><script type="math/tex; mode=display">
a_{jk}=\frac{\sum_{t=1}^{T-1}{\xi_{t}\left( j,k \right)}}{\sum_{t=1}^{T-1}{\gamma_{t}\left( j \right)}}</script><script type="math/tex; mode=display">
b_{j}\left( k \right)=\frac{\sum_{t=1,o_{t}=v_{k}}^{T}{\gamma_{t}\left( j \right)}}{\sum_{t=1}^{T}{\gamma_{t}\left( j \right)}}</script><p>Baum-Welch算法流程如下</p>
<p>输入：观测序列 $O=\left( o_{1},o_{2},…,o_{T} \right)$ </p>
<p>输出：模型参数 $\lambda=\left( \pi,A,B \right)$ </p>
<p>算法流程：</p>
<ol>
<li>$n=0$ ，初始化参数 $\pi_{k}^{\left( 0 \right)}$ ， $a_{jk}^{\left( 0 \right)}$ ， $b_{j}^{\left( 0 \right)}\left( k \right)$ ，组成 $\lambda^{\left( 0 \right)}=\left( \pi^{\left( 0 \right)},A^{\left( 0 \right)},B^{\left( 0 \right)} \right)$ ，其中 $k=1,…,N$ </li>
<li>迭代优化， $n=1,2,…$ ， <script type="math/tex; mode=display">
\pi_{k}^{\left( n+1 \right)}=\gamma_{1}\left( k \right)</script><script type="math/tex; mode=display">
a_{jk}^{\left( n+1 \right)}=\frac{\sum_{t=1}^{T-1}{\xi_{t}\left( j,k \right)}}{\sum_{t=1}^{T-1}{\gamma_{t}\left( j \right)}}</script><script type="math/tex; mode=display">
b_{j}^{\left( n+1 \right)}\left( k \right)=\frac{\sum_{t=1,o_{t}=v_{k}}^{T}{\gamma_{t}\left( j \right)}}{\sum_{t=1}^{T}{\gamma_{t}\left( j \right)}}</script></li>
<li>直到达到最大迭代轮数或者已经达到最优值，结束</li>
</ol>
<h1 id="问题三-预测"><a href="#问题三-预测" class="headerlink" title="问题三  预测"></a>问题三  预测</h1><p>这个问题就是已知一个确定的HMM模型，给出一个观测序列 $O$ ，求出最有可能的状态序列 $I$</p>
<h2 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h2><p>依旧是动态规划思想，先求出最大概率，然后回溯路径上每个节点，得到最有可能的状态序列</p>
<!--[Viterbi算法图示](viterbi.png)-->
<p>如上图，每一个状态的取值都可以是 $q_{1}$ 到 $q_{N}$ 中的一个。先正向计算概率，每次计算完概率后，要保存走到当前节点的概率值，以便于继续向后计算。</p>
<p>例如，计算 $i_{2}$ 到 $i_{3}$ 时，需要遍历 $i_{3}$ 下面的 $q_{1}$ 至 $q_{N}$ 。计算 $q_{k}$ 时，需要找到从 $i_{2}$ 的哪个取值出发，最有可能到达 $i_{3}$ 下的 $q_{k}$ 。比如上图，遍历到 $i_{3}$ 下的 $q_{5}$ ，假如经过计算，到达 $\left( i_{2},q_{2} \right)$ 节点的概率和 $q_{2}$ 到 $q_{5}$ 的转移概率相乘后的结果最大，这个结果就是到达 $\left( i_{3},q_{5} \right)$ 的概率。要到达 $i_{3}$ 的 $q_{5}$ ，就要从 $i_{2}$ 的 $q_{2}$ 出发，也就是说 $\left( i_{3},q_{5} \right)$ 的前趋节点是 $\left( i_{2},q_{2} \right)$ ，记录下前趋节点。</p>
<p>仿照上面的计算思路，一步步向后推进。当到达终点 $i_{T}$ 后，从中找到那个到达概率最大的点，按照记录下的前趋节点，一步步回溯，找到来时的路。</p>
<p>我们定义</p>
<script type="math/tex; mode=display">
\delta_{t}\left( k \right)=\mathop{\max}_{i_{1},...,i_{t-1}}\ P\left( o_{1},...,o_{t},i_{1},...,i_{t-1},i_{t}=q_{k}|\lambda \right)</script><p>其中， $k=1,…,N$ ，推导递推关系</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\delta_{t+1}\left( k \right)&=\mathop{\max}_{i_{1},...,i_{t}}\ P\left( o_{1},...,o_{t+1},i_{1},...,i_{t},i_{t+1}=q_{k}|\lambda \right)\\ 
&=\mathop{\max}_{i_{1},...,i_{t}}\ P\left( o_{t+1}|o_{1},...,o_{t+1},i_{1},...,i_{t},i_{t+1}=q_{k},\lambda \right)P\left( o_{1},...,o_{t},i_{1},...,i_{t},i_{t+1}=q_{k}|\lambda \right)\\ 
&=\mathop{\max}_{i_{1},...,i_{t}}\ P\left( o_{t+1}|i_{t+1}=q_{k},\lambda \right)P\left( i_{t+1}=q_{k}|o_{1},...,o_{t},i_{1},...,i_{t},\lambda \right)P\left( o_{1},...,o_{t},i_{1},...,i_{t}|\lambda \right)\\ 
&=\mathop{\max}_{1\leq j\leq N}\ P\left( o_{t+1}|i_{t+1}=q_{k},\lambda \right)P\left( i_{t+1}=q_{k}|i_{t}=q_{j},\lambda \right)P\left( o_{1},...,o_{t},i_{1},...,i_{t}=q_{j}|\lambda \right)\\ 
&=\mathop{\max}_{1\leq j\leq N}\ b_{k}\left( o_{t+1} \right)\cdot a_{jk}\cdot \delta_{t}\left( j \right)\\ 
&=b_{k}\left( o_{t+1} \right) \cdot\mathop{\max}_{1\leq j\leq N}\ a_{jk} \cdot\delta_{t}\left( j \right)\\ 
\end{aligned}</script><p>解释一下上面的运算。先利用条件概率和联合概率的关系进行提取，再利用两个假设进行运算化简。<br>其中， $t=1,…,T-1$ ，为了帮助理解，给出如下示意图</p>
<!--[Viterbi算法递推关系示意图](viterbi递推.png)-->
<p>如上图，黄色框代表了 $t$ 时刻的计算结果，根据箭头指示的计算方向，递推后添加了粉色框，组合起来代表 $t+1$ 时刻的计算结果。</p>
<p>我们定义</p>
<script type="math/tex; mode=display">
\psi_{t}\left( k \right)=\mathop{\arg\max}_{1\leq j \leq N}\ a_{jk}\cdot\delta_{t-1}\left( j \right)</script><p>其含义为：当 $t$ 时刻状态为 $q_{k}$ 时，使 $b_{k}\left( o_{t+1} \right) \cdot a_{jk} \cdot\delta_{t}\left( j \right)$ 取最大值的 $j$ 的取值（也就是当 $t$ 时刻状态为 $q_{k}$ 时， $t-1$ 时刻的状态），如下图（实线代表最大概率值）。</p>
<!-- [前趋节点记录](viterbi回溯.png) -->
<p>所以，Viterbi算法流程如下</p>
<p>输入：模型参数 $\lambda=\left( \pi,A,B \right)$ ，观测序列 $O=\left( o_{1},o_{2},…,o_{T} \right)$ </p>
<p>输出：观测序列 $I=\left( i_{1},i_{2},…,i_{T} \right)$  </p>
<p>算法流程：</p>
<ol>
<li>$t=1$ ，对 $k=1,…,N$ 遍历计算， <script type="math/tex; mode=display">
\delta_{1}\left( k \right)=\pi_{k}\cdot b_{k}\left( o_{1} \right)</script><script type="math/tex; mode=display">
\psi_{1}\left( k \right)=0</script></li>
<li>递推计算 $t=2,…,T$ 。在每一个 $t$ 中，对 $k=1,…,N$ 遍历计算， <script type="math/tex; mode=display">
\delta_{t}\left( k \right)=b_{k}\left( o_{t} \right) \cdot\mathop{\max}_{1\leq j\leq N}\ a_{jk} \cdot\delta_{t-1}\left( j \right)</script><script type="math/tex; mode=display">
\psi_{t}\left( k \right)=\mathop{\arg\max}_{1\leq j \leq N}\ a_{jk}\cdot\delta_{t-1}\left( j \right)</script></li>
<li>$t=T$ ，递推结束。寻找最大值， <script type="math/tex; mode=display">
P^{*}=\mathop{\max}_{1\leq j\leq N}\ \delta_{T}\left( j \right)</script><script type="math/tex; mode=display">
i_{T}^{*}=\mathop{\arg\max}_{1\leq j \leq N}\ \delta_{T}\left( j \right)</script></li>
<li>回溯 $t=T-1,…,1$ ， <script type="math/tex; mode=display">
i_{t}^{*}=\psi_{t+1}\left( i_{t+1}^{*} \right)</script></li>
<li>最后得到 <script type="math/tex; mode=display">
I=\left( i_{1}^{*},i_{2}^{*},...,i_{T}^{*} \right)</script></li>
</ol>
<h1 id="推荐参考"><a href="#推荐参考" class="headerlink" title="推荐参考"></a>推荐参考</h1><ul>
<li>《统计学习方法（第二版）》</li>
<li><a href="https://www.bilibili.com/video/BV1MW41167Rf">https://www.bilibili.com/video/BV1MW41167Rf</a></li>
<li><a href="https://www.bilibili.com/video/BV1vJ411n7F7?p=9">https://www.bilibili.com/video/BV1vJ411n7F7?p=9</a>  </li>
</ul>
]]></content>
      <categories>
        <category>机器学习算法与模型</category>
      </categories>
      <tags>
        <tag>EM</tag>
        <tag>HMM</tag>
        <tag>马尔可夫</tag>
        <tag>Viterbi</tag>
      </tags>
  </entry>
  <entry>
    <title>最大熵模型（MEM）数学推导思路</title>
    <url>/2020/07/14/MEM%E6%9C%80%E5%A4%A7%E7%86%B5/</url>
    <content><![CDATA[<h1 id="相关定义"><a href="#相关定义" class="headerlink" title="相关定义"></a>相关定义</h1><h2 id="熵的定义"><a href="#熵的定义" class="headerlink" title="熵的定义"></a>熵的定义</h2><p>离散随机变量 $X$ 的概率分布为 $P\left( X \right)$ ，在定义式中 $x$ 是 $X$ 的分量，熵定义为<br><a id="more"></a></p>
<script type="math/tex; mode=display">
H\left( P \right)=-\sum_{x}^{}{P\left( x \right)\log P\left( x \right)}</script><p>可以看出，熵满足</p>
<script type="math/tex; mode=display">
0\leq H\left( P \right)\leq \log\left| X \right|</script><p>其中， $\left| X \right|$ 是 $X$  的取值个数，当且仅当 $X$ 服从均匀分布时，右边等号成立。</p>
<h2 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h2><p>我们得到的数据集为</p>
<script type="math/tex; mode=display">
T=\left\{ \left( x_{i},y_{i} \right) \right\}_{i=1}^{N}</script><p>其中， $x_{i}\in\mathbb{R}^{n}$ 。我们的目标是训练一个模型，对于给定的输入 $X$ ，输出一个分类 $Y$ 。模型认为 $Y$ 是一个随机变量，所以求的就是条件概率分布 $P\left( Y|X \right)$ 。</p>
<p>从已有数据中可以得到一系列经验分布</p>
<script type="math/tex; mode=display">
\tilde P\left( x, y \right)=\tilde P\left( X=x, Y=y \right)=\frac{\nu\left( X=x, Y=y \right)}{N}</script><script type="math/tex; mode=display">
\tilde P\left( x \right)=\tilde P\left( X=x \right)=\frac{\nu\left( X=x \right)}{N}</script><p>其中， $\nu$ 表示计数函数， $N$ 表示样本容量。</p>
<p>我们用特征函数来描述 x 和 y 之间的某种事实关系</p>
<script type="math/tex; mode=display">
f\left( x,y \right)=
\begin{cases} 
1& \text{if }x\text{ and } y\text{ satisfy a certain relation}\\ 
0& \text{else} 
\end{cases}</script><p>特征函数关于经验分布 $\tilde P\left( X,Y \right)$ 的期望为</p>
<script type="math/tex; mode=display">
E_{\tilde P}\left( f \right)=\sum_{x, y}^{}{\tilde P\left( x, y \right)f\left( x,y \right)}</script><p>特征函数关于模型 $P\left( Y|X \right)$ 和经验分布 $\tilde P\left( X \right)$ 的期望为</p>
<script type="math/tex; mode=display">
E_{P}\left( f \right)=\sum_{x, y}^{}{\tilde P\left( x \right)P\left( y|x \right)f\left( x, y \right)}</script><p>当数据中的信息足够多时，可以认为 $E_{\tilde P}\left( f \right)=E_{P}\left( f \right)$ ，也就是</p>
<script type="math/tex; mode=display">
\sum_{x, y}^{}{\tilde P\left( x, y \right)f\left( x,y \right)}=\sum_{x, y}^{}{\tilde P\left( x \right)P\left( y|x \right)f\left( x, y \right)}</script><p>待训练的模型需要满足这个条件。定义条件概率分布 $P\left( Y|X \right)$ 上的条件熵为</p>
<script type="math/tex; mode=display">
H\left( P \right)=-\sum_{x, y}^{}{\tilde P\left( x \right)P\left( y|x \right)\log P\left( y|x \right)}</script><p>最大熵模型，顾名思义，就是在满足约束条件的同时，将条件熵最大化。</p>
<h1 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h1><p>待优化问题为</p>
<script type="math/tex; mode=display">
\left\{ 
\begin{aligned} 
&\max \limits_{P}\ H\left( P \right)=-\sum_{x, y}^{}{\tilde P\left( x \right)P\left( y|x \right)\log P\left( y|x \right)}\\ 
&\mathrm{s.t.}\quad\sum_{y}^{}{P\left( y|x \right)}=1;\quad E_{\tilde P}\left( f_{i} \right)=E_{P}\left( f_{i} \right),\  i=1,2,...,n 
\end{aligned} 
\right.</script><p>改写一下</p>
<script type="math/tex; mode=display">
\left\{ 
\begin{aligned} 
&\min \limits_{P}\ -H\left( P \right)=\sum_{x, y}^{}{\tilde P\left( x \right)P\left( y|x \right)\log P\left( y|x \right)}\\ 
&\mathrm{s.t.}\quad\sum_{y}^{}{P\left( y|x \right)}=1;\quad E_{\tilde P}\left( f_{i} \right)-E_{P}\left( f_{i} \right)=0,\  i=1,2,...,n 
\end{aligned} 
\right.</script><p>接下来需要利用拉格朗日对偶性（支持向量机中用到的方法），转化为无约束最优化问题的对偶问题。首先写出拉格朗日函数</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad L\left( P, w \right)\\ 
&=-H\left( P \right)+w_{0}\left( 1-\sum_{y}^{}{P\left( y|x \right)} \right)+\sum_{i=1}^{n}{w_{i}\left( E_{\tilde P}\left( f_{i} \right)-E_{P}\left( f_{i} \right) \right)}\\ 
&=\sum_{x, y}^{}{\tilde P\left( x \right)P\left( y|x \right)\log P\left( y|x \right)}+w_{0}\left( 1-\sum_{y}^{}{P\left( y|x \right)} \right)\\
&\qquad+\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}^{}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)}-\sum_{x, y}^{}{\tilde P\left( x \right)P\left( y|x \right)f_{i}\left( x, y \right)} \right)}\\ 
\end{aligned}</script><p>根据对偶性</p>
<script type="math/tex; mode=display">
\min\limits_{P}\ \max\limits_{w}\ L\left( P, w \right)\Leftrightarrow \max\limits_{w}\ \min\limits_{P}\ L\left( P, w \right)</script><p>于是，优化问题转化为</p>
<script type="math/tex; mode=display">
\max\limits_{w}\ \min\limits_{P}\ L\left( P, w \right)</script><p>于是先求 $\min\limits_{P}\ L\left( P, w \right)$</p>
<script type="math/tex; mode=display">
P_{w}=\mathop{\arg\min}_{P}\ L\left( P, w \right)=P_{w}\left( y|x \right)</script><p>求偏导数，并令其等于0</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad \frac{\partial L\left( P, w \right)}{\partial P\left( y|x \right)}=0\\ 
&\Rightarrow\sum_{x, y}{\tilde P\left( x \right)\left( 1+\log P\left( y|x \right) \right)}-\sum_{y}{w_{0}}-\sum_{i=1}^{n}{w_{i}\sum_{x, y}{\tilde P\left( x \right)f_{i}\left( x, y \right)}}=0\\ 
&\Rightarrow\sum_{x, y}{\tilde P\left( x \right)\left( 1+\log P\left( y|x \right) \right)}-\sum_{x}{\tilde P\left( x \right)\sum_{y}{w_{0}}}-\sum_{x, y}{\tilde P\left( x \right)\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)}}=0\\ 
&\Rightarrow\sum_{x, y}{\tilde P\left( x \right)\left( 1+\log P\left( y|x \right) \right)}-\sum_{x, y}{\tilde P\left( x \right)w_{0}}-\sum_{x, y}{\tilde P\left( x \right)\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)}}=0\\ 
&\Rightarrow\sum_{x, y}{\tilde P\left( x \right)\left[ 1+\log P\left( y|x \right)-w_{0}-\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right]}=0 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算过程。在第二行到第三行的计算中，运用概率和为1，构造相同的项，为后面提取相同部分做铺垫。</p>
</blockquote>
<p>其中经验分布是从已经存在的样本中统计出来的，所以满足 $\tilde P\left( x \right)&gt;0$ ，于是有</p>
<script type="math/tex; mode=display">
1+\log P\left( y|x \right)-w_{0}-\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)}=0</script><p>解得</p>
<script type="math/tex; mode=display">
P\left( y|x \right)=\exp\left( w_{0}-1+\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)</script><p>又因为有 $\sum_{y}{P\left( y|x \right)}=1$ ，所以</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad\sum_{y}{\exp\left( w_{0}-1+\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)}=1\\ 
&\Rightarrow\exp\left( w_{0}-1 \right)\sum_{y}{\exp\left(\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)}=1\\ 
&\Rightarrow\exp\left( w_{0}-1 \right)=\frac{1}{\sum_{y}{\exp\left(\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)}} 
\end{aligned}</script><p>所以有</p>
<script type="math/tex; mode=display">
\begin{aligned} 
P_{w} 
&=\exp\left( w_{0}-1+\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)\\ 
&=\exp\left( w_{0}-1 \right)\exp\left( \sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)\\ 
&=\frac{\exp\left( \sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)}{Z_{w}\left( x \right)} 
\end{aligned}</script><p>其中， $Z_{w}\left( x \right)$ 为规范化因子</p>
<script type="math/tex; mode=display">
Z_{w}\left( x \right)=\sum_{y}{\exp\left(\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)}</script><p>将 $P_{w}$ 代入拉格朗日函数，就可以求得 $L\left( P, w \right) 关于 P\left( y|x \right)$ 的最小值。于是有</p>
<script type="math/tex; mode=display">
\Psi\left( w \right)=\min\limits_{P}\ L\left( P, w \right)= L\left( P_{w}, w \right)</script><p>继续求解最大化问题</p>
<script type="math/tex; mode=display">
w^{*}=\mathop{\arg\max}_{w}\ \Psi\left( w \right)</script><p>将 $\Psi\left( w \right)$ 展开</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad\Psi\left( w \right)\\ 
&=\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\log P_{w}\left( y|x \right)}+w_{0}\left( 1-\sum_{y}{P_{w}\left( y|x \right)} \right)\\
&\qquad+\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)}-\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)f_{i}\left( x, y \right)} \right)}\\  
\end{aligned}</script><p>由于 $\sum_{y}{P_{w}\left( y|x \right)} =1$ ，代入上式</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad\Psi\left( w \right)\\ 
&=\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\log P_{w}\left( y|x \right)}+\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)}-\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)f_{i}\left( x, y \right)} \right)}\\ 
&=\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\log P_{w}\left( y|x \right)}+\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}-\sum_{i=1}^{n}{w_{i}\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)f_{i}\left( x, y \right)}}\\ 
&=\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\log P_{w}\left( y|x \right)}+\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}-\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)}}\\ 
&=\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}+\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\left[ \log P_{w}\left( y|x \right)-\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right]} 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算过程。运用乘法分配律，移动连加符号，提取相同部分，为后续计算做铺垫。</p>
</blockquote>
<p>由于有</p>
<script type="math/tex; mode=display">
\log P_{w}\left( y|x \right)=\log \frac{\exp\left( \sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)}{Z_{w}\left( x \right)}=\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)}-\log Z_{w}\left( x \right)</script><p>代入上式</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad\Psi\left( w \right)\\ 
&=\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}+\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\left[\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)}-\log Z_{w}\left( x \right)-\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right]}\\ 
&=\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}-\sum_{x, y}{\tilde P\left( x \right)P_{w}\left( y|x \right)\log Z_{w}\left( x \right)}\\ 
&=\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}-\sum_{x}{\tilde P\left( x \right)\log Z_{w}\left( x \right)\sum_{y}{P_{w}\left( y|x \right)}}\\ 
&=\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}-\sum_{x}{\tilde P\left( x \right)\log Z_{w}\left( x \right)}\\ 
&=\sum_{i=1}^{n}{w_{i}\left( \sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)} \right)}-\sum_{x}{\tilde P\left( x \right)\log\left[ \sum_{y}{\exp\left(\sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)} \right]}\\ 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算过程。代入后计算，在第四行到第五行的计算中，利用了概率和为1进行化简。</p>
</blockquote>
<p>发现对数中存在真数相加的情况，且真数中存在待优化参数 $w$ ，直接求导难以解决，需要用特殊的迭代算法：改进的迭代尺度法（IIS）。</p>
<p>关于 $w$ 的似然函数（也就是 $L\left( P_{w},w \right)=\Psi\left( w \right)$ ）写作</p>
<script type="math/tex; mode=display">
\begin{aligned} 
L\left( w \right)
&=\sum_{i=1}^{n}{w_{i} \left(\sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)}\right)}-\sum_{x}{\tilde P\left( x \right)\log Z_{w}\left( x \right)}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{w_{i}f_{i}\left( x,y \right)}}-\sum_{x}{\tilde P\left( x \right)\log Z_{w}\left( x \right)} 
\end{aligned}</script><p>迭代将其最大化，假设从 $w$ 变化为 $w+\delta$ ，似然函数的值的变化为</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad L\left( w+\delta \right)-L\left( w \right)\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\left( w_{i}+\delta_{i} \right)f_{i}\left( x,y \right)}}-\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{w_{i}f_{i}\left( x,y \right)}}\\
&\qquad-\sum_{x}{\tilde P\left( x \right)\log Z_{w+\delta}\left( x \right)}+\sum_{x}{\tilde P\left( x \right)\log Z_{w}\left( x \right)}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}-\sum_{x}{\tilde P\left( x \right)\log\frac{Z_{w+\delta}\left( x \right)}{Z_{w}\left( x \right)}} 
\end{aligned}</script><p>当 $x&gt;0$ 时，不等式</p>
<script type="math/tex; mode=display">
x\geq1 + \ln x</script><p>恒成立， $x=1$ 时等号成立（用导数可以很容易证明）。所以对上面的式子套用不等式</p>
<script type="math/tex; mode=display">
-\log \alpha\geq 1-\alpha</script><p>所以有</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad L\left( w+\delta \right)-L\left( w \right)\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}-\sum_{x}{\tilde P\left( x \right)\log\frac{Z_{w+\delta}\left( x \right)}{Z_{w}\left( x \right)}}\\ 
&\geq \sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+\sum_{x}{\tilde P\left( x \right)\left( 1-\frac{Z_{w+\delta}\left( x \right)}{Z_{w}\left( x \right)} \right)}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+\sum_{x}{\tilde P\left( x \right)}-\sum_{x}{\tilde P\left( x \right)\frac{Z_{w+\delta}\left( x \right)}{Z_{w}\left( x \right)}}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\frac{Z_{w+\delta}\left( x \right)}{Z_{w}\left( x \right)}}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\frac{1}{Z_{w}\left( x \right)}\sum_{y}{\exp\left(\sum_{i=1}^{n}{\left( w_{i}+\delta \right)f_{i}\left( x, y \right)} \right)}}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\frac{1}{Z_{w}\left( x \right)}\sum_{y}{\exp\left(\sum_{i=1}^{n}{ w_{i}f_{i}\left( x, y \right)} +\sum_{i=1}^{n}{ \delta_{i}f_{i}\left( x, y \right)}\right)}}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\frac{1}{Z_{w}\left( x \right)}\sum_{y}{\left[ \exp\left(\sum_{i=1}^{n}{ w_{i}f_{i}\left( x, y \right)} \right)\exp\left( \sum_{i=1}^{n}{ \delta_{i}f_{i}\left( x, y \right)} \right) \right]}}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\sum_{y}{\left[ \frac{1}{Z_{w}\left( x \right)}\exp\left(\sum_{i=1}^{n}{ w_{i}f_{i}\left( x, y \right)} \right)\exp\left( \sum_{i=1}^{n}{ \delta_{i}f_{i}\left( x, y \right)} \right) \right]}}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\sum_{y}{\left[ P_{w}\left( y|x \right)\exp\left( \sum_{i=1}^{n}{ \delta_{i}f_{i}\left( x, y \right)} \right) \right]}}\\ 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算过程。根据上面的不等式放缩后，第四行到第五行利用了概率和为1的性质；之后利用指数乘法的性质对 $\exp$ 函数进行拆解，一路拆解到倒数第二行时，凑出了 </p>
<script type="math/tex; mode=display">
P_{w}\left( y|x \right)=\frac{\exp\left( \sum_{i=1}^{n}{w_{i}f_{i}\left( x, y \right)} \right)}{Z_{w}\left( x \right)}</script><p>的形式，于是在最后一行进行了替换。</p>
</blockquote>
<p>记等号右边部分为</p>
<script type="math/tex; mode=display">
A\left( \delta|w \right)=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\sum_{y}{ P_{w}\left( y|x \right)\exp\left( \sum_{i=1}^{n}{ \delta_{i}f_{i}\left( x, y \right)} \right)}}</script><p>于是有</p>
<script type="math/tex; mode=display">
L\left( w+\delta \right)-L\left( w \right)\geq A\left( \delta|w \right)</script><p>我们定义</p>
<script type="math/tex; mode=display">
f^{\sharp}\left( x,y \right)=\sum_{i}{f_{i}\left( x,y \right)}</script><p>可以继续化简</p>
<script type="math/tex; mode=display">
\begin{aligned} 
&\quad A\left( \delta|w \right)\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\sum_{y}{ P_{w}\left( y|x \right)\exp\left( \sum_{i=1}^{n}{ \delta_{i}f_{i}\left( x, y \right)} \right)}}\\ 
&=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\sum_{y}{ P_{w}\left( y|x \right)\exp\left( f^{\sharp}\left( x,y \right)\sum_{i=1}^{n}{ \frac{\delta_{i}f_{i}\left( x, y \right)}{ f^{\sharp}\left( x,y \right)}} \right)}}\\ 
\end{aligned}</script><blockquote>
<p>解释一下上面的运算。分子分母同乘 $f^{\sharp}\left( x,y \right)$ ，由于 </p>
<script type="math/tex; mode=display">
\sum_{i=1}^{n}{\frac{f_{i}\left( x,y \right)}{f^{\sharp}\left( x,y \right)}}=1</script><p>所以可以在后续使用Jensen不等式。</p>
</blockquote>
<p>利用Jensen不等式进行放缩（和支持向量机中的方法类似）</p>
<script type="math/tex; mode=display">
\exp\left(\sum_{i=1}^{n}{ \frac{f_{i}\left( x, y \right)}{ f^{\sharp}\left( x,y \right)}\delta_{i}f^{\sharp}\left( x,y \right)} \right)\leq \sum_{i=1}^{n}{\frac{f_{i}\left( x, y \right)}{ f^{\sharp}\left( x,y \right)}\exp\left( \delta_{i}f^{\sharp}\left( x,y \right) \right)}</script><p>所以有</p>
<script type="math/tex; mode=display">
A\left( \delta|w \right)\geq \sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\sum_{y}{ P_{w}\left( y|x \right)\sum_{i=1}^{n}{\frac{f_{i}\left( x, y \right)}{ f^{\sharp}\left( x,y \right)}\exp\left( \delta_{i} f^{\sharp}\left( x,y \right) \right)}}}</script><p>记不等号右边为</p>
<script type="math/tex; mode=display">
B\left( \delta|w \right)=\sum_{x, y}{\tilde P\left( x, y \right)\sum_{i=1}^{n}{\delta_{i} f_{i}\left( x,y \right)}}+1-\sum_{x}{\tilde P\left( x \right)\sum_{y}{ P_{w}\left( y|x \right)\sum_{i=1}^{n}{\frac{f_{i}\left( x, y \right)}{ f^{\sharp}\left( x,y \right)}\exp\left( \delta_{i} f^{\sharp}\left( x,y \right) \right)}}}</script><p>所以有</p>
<script type="math/tex; mode=display">
L\left( w+\delta \right)-L\left( w \right)\geq A\left( \delta|w \right)\geq B\left( \delta|w \right)</script><p>为了最大化 $L\left( w+\delta \right)-L\left( w \right)$ ，可以最大化 $B\left( \delta|w \right)$ ，直接求导并令导数为 $0$ 即可</p>
<script type="math/tex; mode=display">
\frac{\partial B\left( \delta|w \right)}{\partial \delta_{i}}=\sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)}-\sum_{x}{\tilde P\left( x \right)\sum_{y}{ P_{w}\left( y|x \right)\sum_{i=1}^{n}{f_{i}\left( x, y \right)\exp\left( \delta_{i} f^{\sharp}\left( x,y \right) \right)}}}=0</script><p>这样，对每个 $\delta_{i}$ 求解方程</p>
<script type="math/tex; mode=display">
g\left( \delta_{i} \right)=\sum_{x}{\tilde P\left( x \right)\sum_{y}{ P_{w}\left( y|x \right)\sum_{i=1}^{n}{f_{i}\left( x, y \right)\exp\left( \delta_{i} f^{\sharp}\left( x,y \right) \right)}}}-\sum_{x, y}{\tilde P\left( x, y \right)f_{i}\left( x,y \right)}=0</script><p>就可以得到 $\delta$</p>
<p>求解方程时可以采用牛顿迭代法，每一轮的迭代过程为</p>
<script type="math/tex; mode=display">
\delta_{i}^{\left( k+1 \right)}=\delta_{i}^{\left( k \right)}-\frac{g\left(\delta_{i}^{\left( k \right)} \right)}{g^{'}\left(\delta_{i}^{\left( k \right)} \right)}</script><p>直到更新的幅度小于一定的阈值，就得到了近似解。</p>
]]></content>
      <categories>
        <category>机器学习算法与模型</category>
      </categories>
      <tags>
        <tag>最大熵</tag>
        <tag>概率图模型</tag>
      </tags>
  </entry>
</search>
